{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from utils.function_clns import config, prepare, subsetting_pipeline\n",
    "import xarray as xr\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "dataset = prepare(subsetting_pipeline( \n",
    "            xr.open_dataarray(os.path.join(config['NDVI']['ndvi_path'], \n",
    "            \"ndvi_smoothed_w2s.nc\")))).transpose(\"time\",\"lon\",\"lat\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "from utils.function_clns import subsetting_pipeline, prepare\n",
    "import numpy as np\n",
    "\n",
    "path = \"/media/BIFROST/N2/Riccardo/MSG/msg_data/NDVI/archive.eumetsat.int/umarf-gwt/onlinedownload/riccardo7/4859700/temp/time/ndvi_eumetsat.nc\"\n",
    "chunks ={'time': -1, \"lat\": 250, \"lon\":250}\n",
    "ds_ndvi = xr.open_dataset(path, engine=\"netcdf4\", chunks=chunks)\n",
    "\n",
    "ds_ndvi = subsetting_pipeline(ds_ndvi).rename({\"Band1\":\"ndvi\"})\n",
    "ds_ndvi[\"ndvi\"] = xr.where(ds_ndvi[\"ndvi\"]==255, np.NaN, ds_ndvi[\"ndvi\"])\n",
    "ndvi = ds_ndvi[\"ndvi\"]/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xesmf as xe\n",
    "ndvi = prepare(ndvi)\n",
    "\n",
    "regridder = xe.Regridder(ndvi, dataset, 'bilinear')\n",
    "\n",
    "# Reproject the entire dataset\n",
    "ds_reprojected = regridder(ndvi)\n",
    "ds_reprojected = ds_reprojected.transpose(\"time\",\"lon\",\"lat\")\n",
    "\n",
    "print(dataset.rio.resolution())\n",
    "print(ds_reprojected.rio.resolution())\n",
    "\n",
    "ds_reprojected = ds_reprojected.drop_duplicates(dim=[\"time\"])\n",
    "#dataset = dataset.sel(time=slice(ds_reprojected.time.min(), ds_reprojected.time.max()))\n",
    "\n",
    "ds_reprojected['time'] = pd.to_datetime(ds_reprojected['time'].values, format='%Y-%m-%d')\n",
    "ds_reprojected['time'] =  ds_reprojected.indexes[\"time\"].normalize()\n",
    "dataset['time'] = pd.to_datetime(dataset['time'].values, format='%Y-%m-%d')\n",
    "dataset['time'] =  dataset.indexes[\"time\"].normalize()\n",
    "\n",
    "# Find the common time values between ds1 and ds2\n",
    "common_times = xr.align(ds_reprojected['time'], dataset['time'])[0].values\n",
    "# Select only the common time values in ds1 and ds2\n",
    "ds1_filtered_1 = dataset.sel(time=common_times)\n",
    "ds_2_filtered_1 = ds_reprojected.sel(time=common_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xskillscore as xs\n",
    "corr_1 = xs.pearson_r(ds1_filtered_1, ds_2_filtered_1, dim=\"time\", skipna=True)\n",
    "rmse_1 = xs.rmse(ds1_filtered_1, ds_2_filtered_1, dim=\"time\", skipna=True)\n",
    "mae_1 = xs.mae(ds1_filtered_1, ds_2_filtered_1, dim=\"time\", skipna=True)\n",
    "mape_1 = xs.mape(ds1_filtered_1, ds_2_filtered_1, dim=\"time\", skipna=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_1.transpose(\"lat\",\"lon\").plot(vmin=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_1.transpose(\"lat\",\"lon\").plot(vmax=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"mean correlation is\", corr_1.mean().values)\n",
    "print(\"mean rmse is\", rmse_1.mean().values)\n",
    "print(\"mean mae is\", mae_1.mean().values)\n",
    "print(\"mean mape is\", mape_1.mean().values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.function_clns import config \n",
    "import os\n",
    "import xarray as xr\n",
    "\n",
    "chunks ={'time': -1, \"lat\": 250, \"lon\":250}\n",
    "\n",
    "path_lai = config[\"LAI\"][\"path\"]\n",
    "lai = xr.open_mfdataset(os.path.join(path_lai, \"*.nc\"), chunks=chunks)\n",
    "\n",
    "from utils.function_clns import subsetting_pipeline\n",
    "lai_ds = subsetting_pipeline(lai)\n",
    "\n",
    "import pandas as pd\n",
    "import xesmf as xe\n",
    "lai_ds = prepare(lai_ds)\n",
    "\n",
    "regridder = xe.Regridder(dataset, lai_ds, 'bilinear')\n",
    "\n",
    "# Reproject the entire dataset\n",
    "ds_reprojected = regridder(dataset)\n",
    "#ds_reprojected = ds_reprojected.transpose(\"time\",\"lon\",\"lat\")\n",
    "\n",
    "print(lai_ds.rio.resolution())\n",
    "print(ds_reprojected.rio.resolution())\n",
    "\n",
    "ds_reprojected = ds_reprojected.drop_duplicates(dim=[\"time\"])\n",
    "lai_ds = lai_ds.drop_duplicates(dim=[\"time\"])\n",
    "\n",
    "#dataset = dataset.sel(time=slice(ds_reprojected.time.min(), ds_reprojected.time.max()))\n",
    "\n",
    "ds_reprojected['time'] = pd.to_datetime(ds_reprojected['time'].values, format='%Y-%m-%d')\n",
    "ds_reprojected['time'] =  ds_reprojected.indexes[\"time\"].normalize()\n",
    "lai_ds['time'] = pd.to_datetime(lai_ds['time'].values, format='%Y-%m-%d')\n",
    "lai_ds['time'] =  lai_ds.indexes[\"time\"].normalize()\n",
    "\n",
    "# Find the common time values between ds1 and ds2\n",
    "common_times = xr.align(ds_reprojected['time'], lai_ds['time'])[0].values\n",
    "# Select only the common time values in ds1 and ds2\n",
    "ds1_filtered = lai_ds.sel(time=common_times).chunk(chunks)\n",
    "ds2_filtered = ds_reprojected.sel(time=common_times).chunk(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NDVI 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr \n",
    "import os\n",
    "from vegetation.preprocessing.ndvi_prep import load_landsaf_ndvi\n",
    "\n",
    "path = \"/media/BIFROST/N2/Riccardo/output\"\n",
    "target_store = \"output_file.zarr\"\n",
    "zarr_path = os.path.join(path, target_store)\n",
    "ds = load_landsaf_ndvi(zarr_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "ds.ndvi_10.isel(time=443).plot(cmap=\"RdYlGn\", vmin=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def group_into_dekads(date_list):\n",
    "    result = []\n",
    "    \n",
    "    # Sort the list of dates to ensure correct grouping\n",
    "    sorted_dates = [pd.to_datetime(t) for t in sorted(date_list)]\n",
    "\n",
    "    threshold_days = [1, 11, 21]\n",
    "    \n",
    "    # Group dates into dekads\n",
    "    current_dekad = []\n",
    "    for idx in range(len(sorted_dates) - 1):  # Iterate up to the second-to-last element\n",
    "        date = sorted_dates[idx]\n",
    "        current_dekad.append(date)\n",
    "        next_day = sorted_dates[idx+1].day\n",
    "        if next_day in threshold_days:\n",
    "            result.append(current_dekad)\n",
    "            current_dekad = []\n",
    "\n",
    "    # Handle the last element separately\n",
    "    last_date = sorted_dates[-1]\n",
    "    current_dekad.append(last_date)\n",
    "    result.append(current_dekad)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_values = dataset[\"time\"].values\n",
    "list_dates = group_into_dekads(time_values)\n",
    "\n",
    "final_dataset = None\n",
    "\n",
    "for date in list_dates:\n",
    "    # Perform the operation on the subset of the dataset for the current date\n",
    "    temp_ds = dataset.sel(time=date).max([\"time\"])\n",
    "    temp_ds[\"time\"] = date[0]\n",
    "\n",
    "    # Append the resulting dataset to the final dataset\n",
    "    if final_dataset is None:\n",
    "        final_dataset = temp_ds\n",
    "    else:\n",
    "        final_dataset = xr.concat([final_dataset, temp_ds], dim=\"time\")\n",
    "\n",
    "# Optionally, you can sort the final dataset by time\n",
    "final_dataset = final_dataset.sortby(\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xesmf as xe\n",
    "from utils.function_clns import prepare\n",
    "\n",
    "print(final_dataset.rio.resolution())\n",
    "print(ds.rio.resolution())\n",
    "\n",
    "chunks ={'time': -1, \"lat\": 250, \"lon\":250}\n",
    "\n",
    "\n",
    "repr_ds = prepare(final_dataset)\n",
    "regridder = xe.Regridder(repr_ds, ds, 'bilinear')\n",
    "\n",
    "# Reproject the entire dataset\n",
    "ds_reprojected = regridder(repr_ds)\n",
    "#ds_reprojected = ds_reprojected.transpose(\"time\",\"lon\",\"lat\")\n",
    "\n",
    "print(ds.rio.resolution())\n",
    "print(ds_reprojected.rio.resolution())\n",
    "\n",
    "ds_reprojected = ds_reprojected.drop_duplicates(dim=[\"time\"])\n",
    "ds = ds.drop_duplicates(dim=[\"time\"])\n",
    "\n",
    "#dataset = dataset.sel(time=slice(ds_reprojected.time.min(), ds_reprojected.time.max()))\n",
    "\n",
    "ds_reprojected['time'] = pd.to_datetime(ds_reprojected['time'].values, format='%Y-%m-%d')\n",
    "ds_reprojected['time'] =  ds_reprojected.indexes[\"time\"].normalize()\n",
    "ds['time'] = pd.to_datetime(ds['time'].values, format='%Y-%m-%d')\n",
    "ds['time'] =  ds.indexes[\"time\"].normalize()\n",
    "\n",
    "# Find the common time values between ds1 and ds2\n",
    "common_times = xr.align(ds_reprojected['time'], ds['time'])[0].values\n",
    "# Select only the common time values in ds1 and ds2\n",
    "ds1_filtered = ds.sel(time=common_times).chunk(chunks)\n",
    "ds2_filtered = ds_reprojected.sel(time=common_times).chunk(chunks)\n",
    "ds2_filtered = xr.where(ds1_filtered.ndvi_10.isnull(), np.NaN, ds2_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xskillscore as xs\n",
    "corr_10 = xs.pearson_r(ds1_filtered.ndvi_10, ds2_filtered, dim=\"time\", skipna=True)\n",
    "rmse_10 = xs.rmse(ds1_filtered.ndvi_10, ds2_filtered, dim=\"time\", skipna=True)\n",
    "mae_10 = xs.mae(ds1_filtered.ndvi_10, ds2_filtered, dim=\"time\", skipna=True)\n",
    "mape_10 = xs.mape(ds1_filtered.ndvi_10, ds2_filtered, dim=\"time\", skipna=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_10.plot(vmin=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_10.plot(vmax=0.4, vmin=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"mean correlation is\", corr_10.mean().values)\n",
    "print(\"mean rmse is\", rmse_10.mean().values)\n",
    "print(\"mean mae is\", mae_10.mean().values)\n",
    "print(\"mean mape is\", mape_10.mean().values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndvi_seviri_mean = ds1_filtered_1.mean([\"lat\",\"lon\"]) ## ours\n",
    "ndvi_eumetsat_mean = ds_2_filtered_1.mean([\"lat\",\"lon\"]) ## eumetsat ndvi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndvi_seviri_mean.plot(color=\"red\")\n",
    "ndvi_eumetsat_mean.plot(color=\"green\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndvi_10_lsaf = ds1_filtered.ndvi_10.mean([\"lat\",\"lon\"]) ## landsaf ndvi\n",
    "ndvi_10 = ds2_filtered.mean([\"lat\",\"lon\"]) ## ours ndvi 10 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds2_filtered.isel(time=0).plot(vmin=0, cmap=\"RdYlGn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1_filtered.ndvi_10.isel(time=0).plot(vmin=0, cmap=\"RdYlGn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.ndvi_10.mean([\"time\"]).plot(vmin=0, cmap=\"RdYlGn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_min=\"2010-01-01\"\n",
    "time_max=\"2010-12-31\"\n",
    "lat = 6\n",
    "lon= 37.5\n",
    "\n",
    "ds.ndvi_10.sel(lon=lon, lat = lat, method=\"nearest\").sel(time=slice(time_min, time_max)).plot(color=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_min=\"2010-01-01\"\n",
    "time_max=\"2010-12-31\"\n",
    "lat = 7.5\n",
    "lon= 36\n",
    "ds2_filtered.sel(lon=lon, lat = lat, method=\"nearest\").sel(time=slice(time_min, time_max)).plot(color=\"red\")\n",
    "ds1_filtered.sel(lon=lon, lat =lat, method=\"nearest\").sel(time=slice(time_min, time_max)).ndvi_10.plot(color=\"green\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vegetation.analysis.indices import compute_vci\n",
    "\n",
    "vci_lsaf = compute_vci(ds1_filtered.ndvi_10)\n",
    "vci_ours = compute_vci(ds2_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_min=\"2018-01-01\"\n",
    "time_max=\"2019-12-31\"\n",
    "lat = 6\n",
    "lon= 39\n",
    "vci_ours.sel(lon=lon, lat = lat, method=\"nearest\").sel(time=slice(time_min, time_max)).plot(color=\"red\")\n",
    "vci_lsaf.sel(lon=lon, lat =lat, method=\"nearest\").sel(time=slice(time_min, time_max)).plot(color=\"green\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis.visualizations.viz_vci_spi import get_subplot_year, box_plot_year\n",
    "years = [i for i in np.arange(2008, 2020)]\n",
    "\n",
    "\n",
    "df_list_1, list_dates_1 = get_subplot_year(ds = ds1_filtered.ndvi_10.to_dataset(name=\"ndvi\"), \n",
    "                                        var=\"ndvi\", year=years)\n",
    "\n",
    "df_list, list_dates = get_subplot_year(ds = ds2_filtered.to_dataset(name=\"ndvi\"), \n",
    "                                        var=\"ndvi\", year=years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_climatology(df_list, list_dates):\n",
    "    import matplotlib.pyplot as plt\n",
    "    from matplotlib import gridspec\n",
    "\n",
    "    fig = plt.figure(figsize=(10,4))\n",
    "    gs = gridspec.GridSpec(1, 2) \n",
    "\n",
    "    # the first subplot\n",
    "    ax0 = fig.add_subplot(gs[0])\n",
    "    #ax0.set_title(\"NDVI for 2009\")\n",
    "    ax0.set_ylabel(\"NDVI value\", fontsize=9)\n",
    "    ax0.set_xlabel(\"Day of the year\", fontsize=9)\n",
    "\n",
    "    line0 = ax0.boxplot(df_list, showfliers=False, \n",
    "                patch_artist=True, \n",
    "                labels=list_dates, \n",
    "                showmeans=False,\n",
    "                medianprops=dict(color=\"green\",ls=\"--\",lw=1), \n",
    "                positions=np.arange(0, len(df_list)*0.1, 0.1),\n",
    "                meanline=True) \n",
    "                # meanprops=dict(color=\"red\", ls=\"-\", lw=2))\n",
    "\n",
    "    n=15\n",
    "    for ax in [ax0]:\n",
    "        ax.set_axisbelow(True)\n",
    "        ax.yaxis.grid(color='grey', linestyle='dashed')\n",
    "        [l.set_visible(False) for (i,l) in enumerate(ax.xaxis.get_ticklabels()) if i % n != 0]\n",
    "        [l.set_visible(False) for (i,l) in enumerate(ax.xaxis.get_gridlines()) if i % n != 0]\n",
    "        ax.tick_params(labelrotation=45, tick1On=False)\n",
    "\n",
    "    for med in line0['medians']:\n",
    "        med.set_color('red')\n",
    "\n",
    "    for box in line0[\"boxes\"] :\n",
    "        box.set_color(\"lightgrey\")\n",
    "        box.set_alpha(0.8)\n",
    "\n",
    "    for whisk in line0[\"whiskers\"]:\n",
    "        whisk.set_color(\"green\")\n",
    "\n",
    "    gs.tight_layout(fig, rect=[0, 0, 1, 0.95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_climatology(df_list_1, list_dates_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_climatology(df_list, list_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndvi_10_lsaf.plot(color=\"green\")\n",
    "ndvi_10.plot(color=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_random_points_over_time(data_array1, data_array2):\n",
    "\n",
    "    lat = data_array1[\"lat\"].values\n",
    "    lon = data_array1[\"lon\"].values\n",
    "\n",
    "    def subset_with_random_points():\n",
    "        # Get random latitude and longitude indices\n",
    "        random_lat = np.random.choice(lat)\n",
    "        random_lon = np.random.choice(lon)\n",
    "\n",
    "        # Extract data for the random points\n",
    "        random_point_data1 = data_array1.sel(lat=random_lat, lon=random_lon)\n",
    "        random_point_data2 = data_array2.sel(lat=random_lat, lon=random_lon)\n",
    "        return random_point_data1, random_point_data2, random_lat, random_lon\n",
    "    \n",
    "    random_point_data1, random_point_data2, random_lat, random_lon = subset_with_random_points()\n",
    "\n",
    "    while random_point_data1.isnull().all() == True:\n",
    "        random_point_data1, random_point_data2, random_lat, random_lon = subset_with_random_points()\n",
    "\n",
    "    # Create a time array\n",
    "    time = data_array1.time.values\n",
    "\n",
    "    # Plot the random points over time\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(time, random_point_data1, label='Reference', marker='o', color=\"green\")\n",
    "    plt.plot(time, random_point_data2, label='Calculated', marker='o',color=\"red\")\n",
    "\n",
    "    # Add labels and legend\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Value')\n",
    "    plt.title(f\"Lat {round(random_lat,2)} Lon {round(random_lon,2)} Over Time\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_random_points_over_time(ds1_filtered.ndvi_10, ds2_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_min = \"2015-01-01\"\n",
    "time_max = \"2019-12-31\"\n",
    "\n",
    "plot_random_points_over_time(ds_2_filtered_1.sel(time=slice(time_min, time_max)), \n",
    "                             ds1_filtered_1.sel(time=slice(time_min, time_max)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_min = \"2015-01-01\"\n",
    "time_max = \"2019-12-31\"\n",
    "\n",
    "plot_random_points_over_time(ds_2_filtered_1.sel(time=slice(time_min, time_max)), \n",
    "                             ds1_filtered_1.sel(time=slice(time_min, time_max)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_min = \"2012-01-01\"\n",
    "time_max = \"2016-12-31\"\n",
    "\n",
    "plot_random_points_over_time(ds_2_filtered_1.sel(time=slice(time_min, time_max)), \n",
    "                             ds1_filtered_1.sel(time=slice(time_min, time_max)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat = 3.13\n",
    "lon = 37.06\n",
    "\n",
    "ds_2_filtered_1.sel(lat=lat, lon=lon, method=\"nearest\").plot(color=\"green\")\n",
    "ds1_filtered_1.sel(lat=lat, lon=lon, method=\"nearest\").plot(color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another lat lon point with good smoothing in the area with high rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat = 0\n",
    "lon = 37.06\n",
    "\n",
    "ds_2_filtered_1.sel(lat=lat, lon=lon, method=\"nearest\").plot(color=\"green\")\n",
    "ds1_filtered_1.sel(lat=lat, lon=lon, method=\"nearest\").plot(color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max NDVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "from utils.function_clns import subsetting_pipeline, prepare\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from vegetation.preprocessing.ndvi_prep import load_eumetsat_ndvi_max\n",
    "\n",
    "\n",
    "filepath = os.path.join(config[\"NDVI\"][\"ndvi_path\"], \"seviri_daily_ndvimax.nc\")\n",
    "max_ndvi = load_eumetsat_ndvi_max(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xesmf as xe\n",
    "max_ndvi = prepare(max_ndvi)\n",
    "\n",
    "regridder = xe.Regridder(max_ndvi, dataset, 'bilinear')\n",
    "\n",
    "# Reproject the entire dataset\n",
    "ds_reprojected = regridder(max_ndvi)\n",
    "ds_reprojected = ds_reprojected.transpose(\"time\",\"lon\",\"lat\")\n",
    "\n",
    "print(dataset.rio.resolution())\n",
    "print(ds_reprojected.rio.resolution())\n",
    "\n",
    "ds_reprojected = ds_reprojected.drop_duplicates(dim=[\"time\"])\n",
    "#dataset = dataset.sel(time=slice(ds_reprojected.time.min(), ds_reprojected.time.max()))\n",
    "\n",
    "ds_reprojected['time'] = pd.to_datetime(ds_reprojected['time'].values, format='%Y-%m-%d')\n",
    "ds_reprojected['time'] =  ds_reprojected.indexes[\"time\"].normalize()\n",
    "dataset['time'] = pd.to_datetime(dataset['time'].values, format='%Y-%m-%d')\n",
    "dataset['time'] =  dataset.indexes[\"time\"].normalize()\n",
    "\n",
    "# Find the common time values between ds1 and ds2\n",
    "common_times = xr.align(ds_reprojected['time'], dataset['time'])[0].values\n",
    "# Select only the common time values in ds1 and ds2\n",
    "ds1_filtered_max = dataset.sel(time=common_times).chunk({\"time\": -1, \"lat\":50, \"lon\":50})\n",
    "ds_2_filtered_max = ds_reprojected.sel(time=common_times).chunk({\"time\":-1, \"lat\":50, \"lon\":50})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_min=\"2013-01-01\"\n",
    "time_max=\"2013-12-31\"\n",
    "lat = 7.5\n",
    "lon= 36\n",
    "ds1_filtered_max.sel(lon=lon, lat = lat, method=\"nearest\").sel(time=slice(time_min, time_max)).plot(color=\"red\")\n",
    "ds_2_filtered_max.sel(lon=lon, lat =lat, method=\"nearest\").sel(time=slice(time_min, time_max)).plot(color=\"green\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_2_filtered_max.sel(lon=lon, lat =lat, method=\"nearest\").sel(time=slice(time_min, time_max)).plot(color=\"green\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1_filtered_max.sel(lon=lon, lat = lat, method=\"nearest\").sel(time=slice(time_min, time_max)).plot(color=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xskillscore as xs\n",
    "corr_max = xs.pearson_r(ds1_filtered_max, ds_2_filtered_max, dim=\"time\", skipna=True)\n",
    "rmse_max = xs.rmse(ds1_filtered_max, ds_2_filtered_max, dim=\"time\", skipna=True)\n",
    "mae_max = xs.mae(ds1_filtered_max, ds_2_filtered_max, dim=\"time\", skipna=True)\n",
    "mape_max = xs.mape(ds1_filtered_max, ds_2_filtered_max, dim=\"time\", skipna=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"mean correlation is\", corr_max.mean().values)\n",
    "print(\"mean rmse is\", rmse_max.mean().values)\n",
    "print(\"mean mae is\", mae_max.mean().values)\n",
    "print(\"mean mape is\", mape_max.mean().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_max.transpose(\"lat\",\"lon\").plot(vmin=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_max.transpose(\"lat\",\"lon\").plot(vmax=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modifying NDVI max "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample calculation function\n",
    "def apply_ndvi_conversion(ndvi_data):\n",
    "    ndvi_data[np.isnan(ndvi_data)] = -2\n",
    "    ndvi_data = np.floor((ndvi_data + 1) * 128)  # [-1 1] -> [0 256]\n",
    "    ndvi_data[ndvi_data > 255] = 255\n",
    "    ndvi_data[ndvi_data < 0] = 0\n",
    "    return np.uint8(ndvi_data)  # change data type from double to uint8\n",
    "\n",
    "def scale_ndvi(ndvi):\n",
    "    scaled_ndvi = ((ndvi + 1) * 127)\n",
    "    scaled_ndvi = xr.where(scaled_ndvi.isnull(), 255, scaled_ndvi)\n",
    "    return scaled_ndvi.astype(int)\n",
    "\n",
    "# Apply the function to the NDVI data variable in the dataset\n",
    "# Create a new dataset with the transformed NDVI values\n",
    "ds_transformed = dataset.copy()\n",
    "\n",
    "ds_converted= scale_ndvi(ds_transformed)\n",
    "ds_converted = xr.where(ds_converted==255, np.NaN, ds_converted)\n",
    "ds_converted = (ds_converted/254)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_converted.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_ndvi = (ds1_filtered_max - ds1_filtered_max.min())/(ds1_filtered_max.max() - ds1_filtered_max.min())\n",
    "zero_ndvi = xr.where(ds1_filtered_max < 0, 0, ds1_filtered_max)\n",
    "\n",
    "### Calculate histogram differences\n",
    "diff_ndvi = ds_converted - ds_2_filtered_max\n",
    "diff_ndvi_orig = ds1_filtered_max - ds_2_filtered_max\n",
    "diff_ndvi_norm = norm_ndvi - ds_2_filtered_max\n",
    "diff_zero_ndvi = zero_ndvi - ds_2_filtered_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_ndvi_orig.plot() ## no modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_zero_ndvi.plot() ### imputing to zero all the negative values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_ndvi_norm.plot()  ### normalizing NDVI from 0 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_ndvi.plot()  ### converting NDVI to 0-255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking correspondence between predicted and real max NDVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_mask = xr.where(corr_max>=0.75, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract predicted and real values\n",
    "predicted_values = ds1_filtered_max.where(cover_ptg<0.6).values.flatten()\n",
    "real_values = ds_2_filtered_max.where(cover_ptg<0.6).values.flatten()\n",
    "\n",
    "arr1 = predicted_values[~np.isnan(predicted_values)]\n",
    "arr2 = real_values[~np.isnan(real_values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create histogram\n",
    "hist1, bins1 = np.histogram(arr1, bins=1000, density=True)\n",
    "# Compute cumulative sum\n",
    "cumulative = np.cumsum(hist1)\n",
    "# Normalize cumulative sum\n",
    "ecdf1 = cumulative / np.max(cumulative)\n",
    "\n",
    "# Create histogram\n",
    "hist2, bins2 = np.histogram(arr2, bins=1000, density=True)\n",
    "# Compute cumulative sum\n",
    "cumulative = np.cumsum(hist2)\n",
    "# Normalize cumulative sum\n",
    "ecdf2 = cumulative / np.max(cumulative)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot ECDF\n",
    "plt.plot(bins1[1:], ecdf1, marker='.', linestyle='-', label='ECDF', color=\"green\")\n",
    "plt.plot(bins2[1:], ecdf2, linestyle='dashed', label='ECDF', color=\"red\")\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('CDF')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_da1 = np.sort(predicted_values)\n",
    "sorted_da2 = np.sort(real_values)\n",
    "import matplotlib.pyplot as plt\n",
    "# Create a quantile plot\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(sorted_da1, sorted_da2, color='blue')\n",
    "plt.xlabel('Quantiles of DataArray 1')\n",
    "plt.ylabel('Quantiles of DataArray 2')\n",
    "plt.title('QQ Plot')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a hexbin plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hexbin(real_values, predicted_values, gridsize=50, bins='log', cmap='Blues')\n",
    "plt.colorbar(label='Count in bin')\n",
    "plt.title('Hexbin Plot of Predicted vs Real Values')\n",
    "plt.xlabel('Real Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a hexbin plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hexbin(real_values, predicted_values, gridsize=50, bins='log', cmap='inferno')\n",
    "plt.colorbar(label='Count in bin')\n",
    "plt.title('Hexbin Plot of Predicted vs Real Values')\n",
    "plt.xlabel('Real Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw vegetation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_xr = xr.open_dataset(os.path.join(\n",
    "    config[\"NDVI\"][\"ndvi_prep\"],\"final_ndvi.nc\"))\n",
    "\n",
    "time_min = ds1_filtered_1.time.min().values\n",
    "time_max = ds1_filtered_1.time.max().values\n",
    "\n",
    "lat = 3.13\n",
    "lon = 37.06\n",
    "res_xr.sel(lat=lat, lon=lon, method=\"nearest\")\\\n",
    "    .sel(time=slice(time_min,time_max))[\"ndvi\"]\\\n",
    "        .plot(label=\"Raw NDVI\", color=\"blue\")\n",
    "\n",
    "ds1_filtered_1.sel(lat=lat, lon=lon, \n",
    "                   method=\"nearest\"\n",
    "                   ).plot(color=\"red\", label=\"Smoothed NDVI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vegetation.analysis.NDVI_indices import compute_ndvi\n",
    "\n",
    "ndvi_1 = compute_ndvi(res_xr[\"channel_1\"], res_xr[\"channel_2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndvi_1.sel(lat=lat, lon=lon, method=\"nearest\").plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_xr.channel_1.sel(lat=lat, lon=lon, method=\"nearest\").plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_xr.channel_2.sel(lat=lat, lon=lon, method=\"nearest\").plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cloudmask cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_path = os.path.join(config['NDVI']['cloud_path'], 'nc_files/new/ndvi_mask.nc')\n",
    "cloud_mask = xr.open_dataarray(cloud_path)\n",
    "print(cloud_mask.rio.resolution())\n",
    "\n",
    "ndvi_raw = xr.open_dataset(os.path.join(config['NDVI']['ndvi_path'], 'ndvi_no_out.nc'))\n",
    "\n",
    "ndvi_raw[\"time\"]= ndvi_raw.indexes[\"time\"].normalize()\n",
    "ndvi_raw = ndvi_raw.drop_duplicates(dim=\"time\")\n",
    "\n",
    "cloud_mask[\"time\"]= cloud_mask.indexes[\"time\"].normalize()\n",
    "cloud_mask = cloud_mask.drop_duplicates(dim=\"time\")\n",
    "\n",
    "# Find the common time values between ds1 and ds2\n",
    "common_times = xr.align(cloud_mask, ndvi_raw)\n",
    "# Select only the common time values in ds1 and ds2\n",
    "cloud_mask = common_times[0]\n",
    "ndvi_raw = common_times[1]\n",
    "\n",
    "ndvi_ds = ndvi_raw[\"ndvi\"].to_dataset()\n",
    "ndvi_ds = ndvi_ds.assign(cloud_mask = cloud_mask)\n",
    "\n",
    "### calculate rolling cloud and % of cloud measures\n",
    "rolling_cloud = xr.where(cloud_mask==2,1,0)\\\n",
    "    .rolling(time=180).sum()\n",
    "\n",
    "cl_mk =xr.where(cloud_mask==2,1,0)\n",
    "cover_ptg = (cl_mk.sum([\"time\"])/cl_mk.count([\"time\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cover_ptg.where(cover_ptg<0.6).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cover_ptg.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xskillscore as xs\n",
    "corr_max_clean = xs.pearson_r(ds1_filtered_max.where(cover_ptg<0.6), \n",
    "                        ds_2_filtered_max.where(cover_ptg<0.6), dim=\"time\", skipna=True)\n",
    "\n",
    "rmse_max_clean = xs.rmse(ds1_filtered_max.where(cover_ptg<0.6), \n",
    "                        ds_2_filtered_max.where(cover_ptg<0.6), dim=\"time\", skipna=True)\n",
    "print(\"mean correlation is\", corr_max_clean.mean().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"mean rmse is\", rmse_max_clean.mean().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_max_clean.transpose(\"lat\", \"lon\").plot(vmax=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_max_clean.transpose(\"lat\", \"lon\").plot(vmin=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lon = 36\n",
    "lat = 7.5\n",
    "time_min = \"2015-01-01\"\n",
    "time_max = \"2016-01-01\"\n",
    "ndvi_raw.sel(lon=lon, lat = lat, method=\"nearest\").sel(time=slice(time_min, time_max)).channel_1.plot(color=\"red\")\n",
    "ds1_filtered_max.sel(lon=lon, lat =lat, method=\"nearest\").sel(time=slice(time_min, time_max)).plot(color=\"blue\")\n",
    "ndvi_raw.sel(lon=lon, lat = lat, method=\"nearest\").sel(time=slice(time_min, time_max)).channel_2.plot(color=\"yellow\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lon = 37.5\n",
    "lat = 12.5\n",
    "time_min = \"2015-01-01\"\n",
    "time_max = \"2016-01-01\"\n",
    "ndvi_raw.sel(lon=lon, lat = lat, method=\"nearest\").sel(time=slice(time_min, time_max)).channel_1.plot(color=\"red\")\n",
    "ds1_filtered_max.sel(lon=lon, lat =lat, method=\"nearest\").sel(time=slice(time_min, time_max)).plot(color=\"green\")\n",
    "ndvi_raw.sel(lon=lon, lat = lat, method=\"nearest\").sel(time=slice(time_min, time_max)).channel_2.plot(color=\"yellow\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing smoother"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hl_envelopes_idx(s, dmin=1, dmax=1, split=False):\n",
    "    \"\"\"\n",
    "    Input :\n",
    "    s: 1d-array, data signal from which to extract high and low envelopes\n",
    "    dmin, dmax: int, optional, size of chunks, use this if the size of the input signal is too big\n",
    "    split: bool, optional, if True, split the signal in half along its mean, might help to generate the envelope in some cases\n",
    "    Output :\n",
    "    lmin,lmax : high/low envelope idx of input signal s\n",
    "    \"\"\"\n",
    "    # locals min      \n",
    "    lmin = (np.diff(np.sign(np.diff(s))) > 0).nonzero()[0] + 1 \n",
    "    # locals max\n",
    "    lmax = (np.diff(np.sign(np.diff(s))) < 0).nonzero()[0] + 1 \n",
    "    if split:\n",
    "        # s_mid is zero if s centered around x-axis or more generally mean of signal\n",
    "        s_mid = np.mean(s) \n",
    "        # pre-sorting of locals min based on relative position with respect to s_mid \n",
    "        lmin = lmin[s[lmin][s_mid]]\n",
    "    # global max of dmax-chunks of locals max \n",
    "    lmin = lmin[[i+np.argmin(s[lmin[i:i+dmin]]) for i in range(0,len(lmin),dmin)]]\n",
    "    # global min of dmin-chunks of locals min \n",
    "    lmax = lmax[[i+np.argmax(s[lmax[i:i+dmax]]) for i in range(0,len(lmax),dmax)]]\n",
    "    \n",
    "    return lmin,lmax\n",
    "\n",
    "def interpolate_nan(array_like):\n",
    "    array = array_like.copy()\n",
    "\n",
    "    isnan_array = ~np.isnan(array)\n",
    "\n",
    "    xp = isnan_array.ravel().nonzero()[0]\n",
    "\n",
    "    fp = array[~np.isnan(array)]\n",
    "    x = np.isnan(array).ravel().nonzero()[0]\n",
    "\n",
    "    array[np.isnan(array)] = np.interp(x, xp, fp)\n",
    "\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ED_SG(series, formatted_dates=None, plot=True):\n",
    "    from scipy.signal import savgol_filter\n",
    "    import matplotlib.dates as mdates\n",
    "\n",
    "    low_idx, high_idx = hl_envelopes_idx(series)\n",
    "    \n",
    "    # Create a boolean mask indicating which indices to nullify\n",
    "    mask = np.zeros_like(series, dtype=bool)\n",
    "    mask[high_idx] = True\n",
    "    \n",
    "    # Use np.where() to preserve values at certain indices\n",
    "    masked_arr = np.where(np.isin(np.arange(len(series)), high_idx), series, np.nan)\n",
    "    \n",
    "    new_arr = interpolate_nan(masked_arr)\n",
    "    \n",
    "    smoothed_array = savgol_filter(new_arr, 15, 3, axis=0)\n",
    "    \n",
    "    if plot is True:\n",
    "        # plot\n",
    "        # plt.plot(formatted_dates,y_corr,label='signal')\n",
    "        # plt.plot(formatted_dates[high_idx], y_corr[high_idx], 'r', label='low')\n",
    "        plt.plot(formatted_dates, new_arr , color= 'grey', label='high')\n",
    "        plt.plot(formatted_dates, smoothed_array, 'blue', label='ED-SG')\n",
    "    \n",
    "        # Define the date formatter and set the interval for major ticks\n",
    "        date_format = mdates.DateFormatter('%Y-%m')\n",
    "        plt.gca().xaxis.set_major_formatter(date_format)\n",
    "        plt.gca().xaxis.set_major_locator(mdates.DayLocator(interval=90))\n",
    "    \n",
    "        plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "        plt.legend()\n",
    "    \n",
    "        plt.show()\n",
    "\n",
    "    return smoothed_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series = y \n",
    "\n",
    "low_idx, high_idx = hl_envelopes_idx(series)\n",
    "    \n",
    "# Create a boolean mask indicating which indices to nullify\n",
    "mask = np.zeros_like(series, dtype=bool)\n",
    "mask[high_idx] = True\n",
    "\n",
    "# Use np.where() to preserve values at certain indices\n",
    "masked_arr = np.where(np.isin(np.arange(len(series)), high_idx), series, np.nan)\n",
    "\n",
    "new_arr = interpolate_nan(masked_arr)\n",
    "\n",
    "smoothed_array = savgol_filter(new_arr, 15, 3, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modape.whittaker import ws2doptv, ws2d, ws2doptvp\n",
    "from array import array\n",
    "from datetime import datetime\n",
    "import matplotlib.dates as mdates\n",
    "from scipy.signal import savgol_filter, hilbert, chirp\n",
    "import numpy as np\n",
    "\n",
    "lon = 36.75\n",
    "lat = -0.1\n",
    "time_min = \"2015-01-01\"\n",
    "time_max = \"2020-01-01\"\n",
    "\n",
    "y_temp = ndvi_raw.where(cloud_mask!=2, np.NaN).ndvi\\\n",
    "    .sel(lon=lon, lat = lat, method=\"nearest\")\\\n",
    "    .sel(time=slice(time_min, time_max))\n",
    "\n",
    "y = y_temp.values.astype(np.float32)\n",
    "y_corr = np.where(np.isnan(y), 0, y) \n",
    "w = np.where(np.isnan(y), 0, 1)\n",
    "\n",
    "dates = pd.to_datetime(y_temp.time.values)\n",
    "formatted_dates = mdates.date2num(dates)\n",
    "\n",
    "p = 0.99 \n",
    "lambda_min=-2\n",
    "lambda_max = 4\n",
    "\n",
    "w_corr = w.astype(np.double)\n",
    "y_corr_db = y_corr.astype(np.double)\n",
    "\n",
    "z, sopt = ws2doptvp(y_corr_db, w_corr, \n",
    "                    array(\"d\", np.arange(lambda_min, lambda_max, 0.2).round(2)), \n",
    "                    p=p)\n",
    "z_arr =  np.array(z, dtype=np.float32)\n",
    "\n",
    "ch1 = ndvi_raw.channel_1.sel(lon=lon, lat = lat, method=\"nearest\")\\\n",
    "    .sel(time=slice(time_min, time_max))\n",
    "\n",
    "ch2 = ndvi_raw.channel_2.sel(lon=lon, lat = lat, method=\"nearest\")\\\n",
    "    .sel(time=slice(time_min, time_max))\n",
    "\n",
    "sg = ED_SG(y, plot=False)\n",
    "\n",
    "# Define the date formatter and set the interval for major ticks\n",
    "date_format = mdates.DateFormatter('%Y-%m')\n",
    "plt.gca().xaxis.set_major_formatter(date_format)\n",
    "plt.gca().xaxis.set_major_locator(mdates.DayLocator(interval=90))\n",
    "\n",
    "plt.plot(formatted_dates, z_arr, color = \"green\", alpha=0.7, label=\"WS\")\n",
    "plt.plot(formatted_dates, y, color = \"black\", alpha=1, label= \"NDVI\")\n",
    "plt.plot(formatted_dates, ch1, color=\"violet\", alpha=0.6 )\n",
    "plt.plot(formatted_dates, ch2, color=\"red\", alpha=0.3 )\n",
    "plt.plot(formatted_dates, sg, color = \"grey\", alpha=0.5, label=\"ED-SG\")\n",
    "\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streaks = []\n",
    "current_streak = 0\n",
    "for day in y:\n",
    "    if np.isnan(day):\n",
    "        current_streak += 1\n",
    "    else:\n",
    "        if current_streak > 0:\n",
    "            streaks.append(current_streak)\n",
    "            current_streak = 0\n",
    "\n",
    "if current_streak > 0:\n",
    "    streaks.append(current_streak)\n",
    "\n",
    "# Step 2: Calculate length of each streak\n",
    "total_length = sum(streaks)\n",
    "num_streaks = len(streaks)\n",
    "\n",
    "# Step 3: Calculate average\n",
    "average_length = total_length / num_streaks\n",
    "print(\"Average length of consecutive days with clouds:\", round(average_length,1))\n",
    "print(\"Max number of consecutive days with clouds:\", max(streaks))\n",
    "print(\"Std of consecutive days with clouds:\", np.std(streaks))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import obspy\n",
    "import obspy.signal.filter\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import savgol_filter, hilbert, chirp\n",
    "import numpy as np\n",
    "\n",
    "analytic_signal = hilbert(y)\n",
    "amplitude_envelope = np.abs(analytic_signal)\n",
    "\n",
    "data_envelope = obspy.signal.filter.envelope(y)\n",
    "smoothed_array = savgol_filter(data_envelope, 15, 3, axis=0)\n",
    "\n",
    "# plt.plot(data_envelope, color = \"blue\", alpha=0.4)\n",
    "plt.plot(y, color = \"grey\", alpha=0.3)\n",
    "#plt.plot(smoothed_array, color=\"blue\", alpha=0.5)\n",
    "plt.plot(z_arr, color = \"green\", alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "user = config[\"EARTHDATA\"][\"user\"]\n",
    "password = config[\"EARTHDATA\"][\"password\"]\n",
    "\n",
    "\n",
    "targetdir = Path(os.path.join(config[\"MODIS\"][\"download\"], \"MASK\"))\n",
    "\n",
    "if not os.path.exists(targetdir):\n",
    "    os.makedirs(targetdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load MODIS NDVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.xarray_functions import add_time_tiff\n",
    "import os\n",
    "import xarray as xr\n",
    "from utils.function_clns import subsetting_pipeline, prepare, config\n",
    "\n",
    "chunks = {\"time\":\"auto\", \"lat\":\"auto\", \"lon\":\"auto\"}\n",
    "\n",
    "modis_path = os.path.join(config[\"MODIS\"][\"download\"], \"ndvi_daily_modis.zarr\")\n",
    "\n",
    "if os.path.isfile(modis_path):\n",
    "    modis_ds = xr.open_zarr(modis_path, chunks=chunks)\n",
    "else:\n",
    "    modis_files_path = os.path.join(config[\"MODIS\"][\"download\"], \"ref061_NDVI\")\n",
    "    files = [os.path.join(modis_files_path, f) for f in os.listdir(modis_files_path) if f.endswith(\".tif\")]\n",
    "    ds = xr.open_mfdataset(files,engine=\"rasterio\", \n",
    "                           parallel=True, \n",
    "                           preprocess=add_time_tiff,\n",
    "                           chunks=chunks)\n",
    "    ds = ds.isel(band=0)\n",
    "    ds = ds.rename({\"band_data\":\"ndvi\"})\n",
    "    modis_ds = ds.drop(\"band\")\n",
    "\n",
    "from dask.diagnostics import ProgressBar\n",
    "with ProgressBar():\n",
    "    modis_ds.to_zarr(os.path.join(config[\"MODIS\"][\"download\"], \"ndvi_daily_modis.zarr\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create SEVIRI composite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/woden/anaconda3/envs/ric_gis_py39/lib/python3.9/abc.py:85: FutureWarning: xarray subclass XarrayWS should explicitly define __slots__\n",
      "  cls = super().__new__(mcls, name, bases, namespace, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "from utils.xarray_functions import add_time, compute_radiance\n",
    "from vegetation.analysis.indices import compute_ndvi\n",
    "from utils.function_clns import subsetting_pipeline\n",
    "from utils.function_clns import config\n",
    "import logging\n",
    "from dask.diagnostics import ProgressBar\n",
    "import pyproj\n",
    "from vegetation.preprocessing.ndvi_prep import apply_seviri_cloudmask, remove_ndvi_outliers, NDVIPreprocess\n",
    "\n",
    "chunks = {\"time\":\"auto\", \"lat\":\"auto\", \"lon\":\"auto\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10_30', '09_15', '12_15']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "base_dir = config[\"SEVIRI\"][\"download\"]\n",
    "cloud_dir = config[\"NDVI\"][\"cloud_path\"]\n",
    "hour_pattern = re.compile(r'\\d{2}_\\d{2}')\n",
    "hour_folders = [folder for folder in os.listdir(base_dir) if hour_pattern.match(folder)]\n",
    "hour_folders = [hour_folders[1], hour_folders[0],hour_folders[2]]\n",
    "hour_folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception [Errno -101] NetCDF: HDF error: '/media/BIFROST/N2/Riccardo/MSG/msg_data/10_30/HRSEVIRI_20190205T104509Z_20190205T105743Z_epct_3f4b9864_FPC.nc' on files 10_30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping corrupted file: /media/BIFROST/N2/Riccardo/MSG/msg_data/10_30/HRSEVIRI_20190205T104509Z_20190205T105743Z_epct_3f4b9864_FPC.nc. Error: [Errno -101] NetCDF: HDF error: '/media/BIFROST/N2/Riccardo/MSG/msg_data/10_30/HRSEVIRI_20190205T104509Z_20190205T105743Z_epct_3f4b9864_FPC.nc'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Starting applying cloudmask...\n",
      "INFO:root:Successfully applied cloudmask\n",
      "INFO:root:Starting applying cloudmask...\n",
      "INFO:root:Successfully applied cloudmask\n",
      "ERROR:root:Exception did not find a match in any of xarray's currently installed IO backends ['netcdf4', 'h5netcdf', 'scipy', 'ee', 'kerchunk', 'rasterio', 'zarr']. Consider explicitly selecting one of the installed engines via the ``engine`` parameter, or installing additional IO dependencies, see:\n",
      "https://docs.xarray.dev/en/stable/getting-started-guide/installing.html\n",
      "https://docs.xarray.dev/en/stable/user-guide/io.html on files 12_15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping corrupted file: /media/BIFROST/N2/Riccardo/MSG/msg_data/12_15/HRSEVIRI_20151018T121510Z_20151018T122741Z_epct_69dc5933_FPC.nc. Error: did not find a match in any of xarray's currently installed IO backends ['netcdf4', 'h5netcdf', 'scipy', 'ee', 'kerchunk', 'rasterio', 'zarr']. Consider explicitly selecting one of the installed engines via the ``engine`` parameter, or installing additional IO dependencies, see:\n",
      "https://docs.xarray.dev/en/stable/getting-started-guide/installing.html\n",
      "https://docs.xarray.dev/en/stable/user-guide/io.html\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "dimension 'time' already exists as a scalar variable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m     ds_temp \u001b[38;5;241m=\u001b[39m read_netcdfs(ds_files, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m\"\u001b[39m, transform_func\u001b[38;5;241m=\u001b[39mpreprocess_file)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 30\u001b[0m     ds_cl \u001b[38;5;241m=\u001b[39m \u001b[43mxr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_mfdataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcl_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreprocess_cloud\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     logging\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mException \u001b[39m\u001b[38;5;132;01m{e}\u001b[39;00m\u001b[38;5;124m on files \u001b[39m\u001b[38;5;132;01m{f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(e\u001b[38;5;241m=\u001b[39me, f\u001b[38;5;241m=\u001b[39mcloud_path\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]))\n",
      "File \u001b[0;32m~/anaconda3/envs/ric_gis_py39/lib/python3.9/site-packages/xarray/backends/api.py:1059\u001b[0m, in \u001b[0;36mopen_mfdataset\u001b[0;34m(paths, chunks, concat_dim, compat, preprocess, engine, data_vars, coords, combine, parallel, join, attrs_file, combine_attrs, **kwargs)\u001b[0m\n\u001b[1;32m   1057\u001b[0m closers \u001b[38;5;241m=\u001b[39m [getattr_(ds, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_close\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m ds \u001b[38;5;129;01min\u001b[39;00m datasets]\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m preprocess \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1059\u001b[0m     datasets \u001b[38;5;241m=\u001b[39m [preprocess(ds) \u001b[38;5;28;01mfor\u001b[39;00m ds \u001b[38;5;129;01min\u001b[39;00m datasets]\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m parallel:\n\u001b[1;32m   1062\u001b[0m     \u001b[38;5;66;03m# calling compute here will return the datasets/file_objs lists,\u001b[39;00m\n\u001b[1;32m   1063\u001b[0m     \u001b[38;5;66;03m# the underlying datasets will still be stored as dask arrays\u001b[39;00m\n\u001b[1;32m   1064\u001b[0m     datasets, closers \u001b[38;5;241m=\u001b[39m dask\u001b[38;5;241m.\u001b[39mcompute(datasets, closers)\n",
      "File \u001b[0;32m~/anaconda3/envs/ric_gis_py39/lib/python3.9/site-packages/xarray/backends/api.py:1059\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1057\u001b[0m closers \u001b[38;5;241m=\u001b[39m [getattr_(ds, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_close\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m ds \u001b[38;5;129;01min\u001b[39;00m datasets]\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m preprocess \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1059\u001b[0m     datasets \u001b[38;5;241m=\u001b[39m [\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m ds \u001b[38;5;129;01min\u001b[39;00m datasets]\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m parallel:\n\u001b[1;32m   1062\u001b[0m     \u001b[38;5;66;03m# calling compute here will return the datasets/file_objs lists,\u001b[39;00m\n\u001b[1;32m   1063\u001b[0m     \u001b[38;5;66;03m# the underlying datasets will still be stored as dask arrays\u001b[39;00m\n\u001b[1;32m   1064\u001b[0m     datasets, closers \u001b[38;5;241m=\u001b[39m dask\u001b[38;5;241m.\u001b[39mcompute(datasets, closers)\n",
      "Cell \u001b[0;32mIn[3], line 9\u001b[0m, in \u001b[0;36mpreprocess_cloud\u001b[0;34m(ds)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_cloud\u001b[39m(ds):\n\u001b[0;32m----> 9\u001b[0m     ds \u001b[38;5;241m=\u001b[39m \u001b[43madd_time\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m/media/BIFROST/N2/Riccardo/Indices_analysis/src/utils/xarray_functions.py:126\u001b[0m, in \u001b[0;36madd_time\u001b[0;34m(xr_df)\u001b[0m\n\u001b[1;32m    124\u001b[0m date_xr \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mstrptime(my_date_string,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mSZ\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m#datetime.strptime(my_date_string, '%Y%m%d/%H:%M')\u001b[39;00m\n\u001b[1;32m    125\u001b[0m date_xr \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(date_xr)\n\u001b[0;32m--> 126\u001b[0m xr_df \u001b[38;5;241m=\u001b[39m \u001b[43mxr_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massign_coords\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_xr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m xr_df \u001b[38;5;241m=\u001b[39m xr_df\u001b[38;5;241m.\u001b[39mexpand_dims(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xr_df\n",
      "File \u001b[0;32m~/anaconda3/envs/ric_gis_py39/lib/python3.9/site-packages/xarray/core/common.py:627\u001b[0m, in \u001b[0;36mDataWithCoords.assign_coords\u001b[0;34m(self, coords, **coords_kwargs)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    625\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_calc_assign_results(coords_combined)\n\u001b[0;32m--> 627\u001b[0m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoords\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/envs/ric_gis_py39/lib/python3.9/site-packages/xarray/core/coordinates.py:566\u001b[0m, in \u001b[0;36mCoordinates.update\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;66;03m# special case for PandasMultiIndex: updating only its dimension coordinate\u001b[39;00m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;66;03m# is still allowed but depreciated.\u001b[39;00m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;66;03m# It is the only case where we need to actually drop coordinates here (multi-index levels)\u001b[39;00m\n\u001b[1;32m    563\u001b[0m \u001b[38;5;66;03m# TODO: remove when removing PandasMultiIndex's dimension coordinate.\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_drop_coords(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_names \u001b[38;5;241m-\u001b[39m coords_to_align\u001b[38;5;241m.\u001b[39m_names)\n\u001b[0;32m--> 566\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_coords\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcoords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ric_gis_py39/lib/python3.9/site-packages/xarray/core/coordinates.py:751\u001b[0m, in \u001b[0;36mDatasetCoordinates._update_coords\u001b[0;34m(self, coords, indexes)\u001b[0m\n\u001b[1;32m    748\u001b[0m variables\u001b[38;5;241m.\u001b[39mupdate(coords)\n\u001b[1;32m    750\u001b[0m \u001b[38;5;66;03m# check for inconsistent state *before* modifying anything in-place\u001b[39;00m\n\u001b[0;32m--> 751\u001b[0m dims \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_dimensions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m new_coord_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(coords)\n\u001b[1;32m    753\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dim, size \u001b[38;5;129;01min\u001b[39;00m dims\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/anaconda3/envs/ric_gis_py39/lib/python3.9/site-packages/xarray/core/variable.py:2940\u001b[0m, in \u001b[0;36mcalculate_dimensions\u001b[0;34m(variables)\u001b[0m\n\u001b[1;32m   2938\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dim, size \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(var\u001b[38;5;241m.\u001b[39mdims, var\u001b[38;5;241m.\u001b[39mshape):\n\u001b[1;32m   2939\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dim \u001b[38;5;129;01min\u001b[39;00m scalar_vars:\n\u001b[0;32m-> 2940\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2941\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdimension \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdim\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m already exists as a scalar variable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2942\u001b[0m         )\n\u001b[1;32m   2943\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dim \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m dims:\n\u001b[1;32m   2944\u001b[0m         dims[dim] \u001b[38;5;241m=\u001b[39m size\n",
      "\u001b[0;31mValueError\u001b[0m: dimension 'time' already exists as a scalar variable"
     ]
    }
   ],
   "source": [
    "from utils.function_clns import read_netcdfs\n",
    "\n",
    "def preprocess_file(ds):\n",
    "    ds = add_time(ds)\n",
    "    ds = compute_radiance(ds)\n",
    "    return ds\n",
    "\n",
    "def preprocess_cloud(ds):\n",
    "    if \"time\" not in ds.dims:\n",
    "        ds = add_time(ds)\n",
    "    return ds\n",
    "\n",
    "# List to store paths to NetCDF files\n",
    "datasets= []\n",
    "\n",
    "# Loop through hour folders\n",
    "for hour_folder in hour_folders:\n",
    "    folder_path = os.path.join(base_dir, hour_folder)\n",
    "    cloud_path = os.path.join(cloud_dir, hour_folder)\n",
    "    \n",
    "    # Filter NetCDF files\n",
    "    ds_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.nc')]\n",
    "    cl_files = [os.path.join(cloud_path, f) for f in os.listdir(cloud_path) if f.endswith('.nc')]\n",
    "\n",
    "    try:\n",
    "        ds_temp = xr.open_mfdataset(ds_files, parallel=False, chunks=chunks, preprocess=preprocess_file)\n",
    "    except Exception as e:\n",
    "        logging.error(\"Exception {e} on files {f}\".format(e=e, f=folder_path.split(\"/\")[-1]))\n",
    "        ds_temp = read_netcdfs(ds_files, dim=\"time\", transform_func=preprocess_file)\n",
    "    try:\n",
    "        ds_cl = xr.open_mfdataset(cl_files, parallel=False, chunks=chunks, preprocess=preprocess_cloud)\n",
    "    except KeyError:\n",
    "        logging.error(\"Exception {e} on files {f}\".format(e=e, f=cloud_path.split(\"/\")[-1]))\n",
    "        cl_files = read_netcdfs(cl_files, dim=\"time\", transform_func=preprocess_cloud)\n",
    "\n",
    "    logging.info(\"Starting applying cloudmask...\")\n",
    "    temp_ds = apply_seviri_cloudmask(ds_temp, ds_cl, align=False)\n",
    "    datasets.append(temp_ds)\n",
    "    logging.info(\"Successfully applied cloudmask\")\n",
    "\n",
    "# Open all NetCDF files using xarray.open_mfdataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"ndvi_full_image.zarr\"\n",
    "\n",
    "baseline_path = config[\"DEFAULT\"][\"local\"] #config[\"NDVI\"][\"ndvi_path\"]\n",
    "os.environ['PROJ_LIB'] = pyproj.datadir.get_data_dir()\n",
    "\n",
    "ds_915 = xr.open_zarr(os.path.join(config[\"NDVI\"][\"ndvi_path\"], filename), \n",
    "                        chunks=chunks)\n",
    "\n",
    "def _preprocess(ds):\n",
    "    from utils.xarray_functions import add_time\n",
    "    ds = add_time(ds)\n",
    "    return ds\n",
    "\n",
    "cloud_path = os.path.join(config[\"NDVI\"][\"cloud_path\"], \"09_15\")\n",
    "files = [os.path.join(cloud_path, file) for file in os.listdir(cloud_path) if file.endswith(\".nc\")]\n",
    "with ProgressBar():\n",
    "    cl_915 = xr.open_mfdataset(files, \n",
    "                        preprocess=_preprocess, \n",
    "                        engine='netcdf4', \n",
    "                        parallel=False, chunks=chunks)\n",
    "\n",
    "logging.info(\"Loaded cloudmask\")\n",
    "ds_915 = apply_seviri_cloudmask(ds_915, cl_915)\n",
    "logging.info(\"Applied cloudmask\")\n",
    "ds_915[\"ndvi\"] = remove_ndvi_outliers(ds_915[\"ndvi\"])\n",
    "ds_915 = NDVIPreprocess(ds_915[\"ndvi\"]).get_processed_data()\n",
    "\n",
    "path = \"/media/BIFROST/N2/Riccardo/MSG/msg_data/12_30/batch_2/processed/new_process\"\n",
    "files = [os.path.join(path, f) for f in os.listdir(path) if f.endswith(\".nc\")]\n",
    "ds_12 = xr.open_mfdataset(files, chunks=chunks, parallel=False)\n",
    "\n",
    "batch3_path = \"/media/BIFROST/N2/Riccardo/MSG/msg_data/12_30/batch_3/*.nc\"\n",
    "\n",
    "def preprocess(ds):\n",
    "    ds = add_time(ds)\n",
    "    ds = compute_radiance(ds)\n",
    "    return ds \n",
    "\n",
    "ds_new = xr.open_mfdataset(batch3_path, preprocess =preprocess, parallel=True, chunks=chunks)\n",
    "ds_new = compute_ndvi(band2=ds_new[\"channel_2\"],band1=ds_new[\"channel_1\"])\n",
    "ds_new = subsetting_pipeline(ds_new)\n",
    "ds_1230 = xr.concat([ds_12[\"ndvi\"], ds_new], dim=\"time\")\n",
    "\n",
    "### load and prepare cloudmask\n",
    "base_dir = 'batch_2/nc_files/new/ndvi_mask.nc'\n",
    "cl_df = xr.open_dataset(os.path.join(config['NDVI']['cloud_path'], base_dir), chunks=chunks)\n",
    "cl_df = cl_df.sel(time=slice(cl_df['time'].min(), '2021-01-01'))\n",
    "\n",
    "ds_1230 = apply_seviri_cloudmask(ds_1230, cl_df)\n",
    "ds_1230 = remove_ndvi_outliers(ds_1230)\n",
    "ds_1230 = NDVIPreprocess(ds_1230).get_processed_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_agg = xr.concat([ds_1230, ds_915], dim=\"time\")\n",
    "#ds_aggr = ds_aggr.groupby(\"time\").max([\"time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_df.rio.resolution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rasterio.enums import Resampling\n",
    "with ProgressBar():\n",
    "    repr_ds = ds_915.rio.reproject_match(ds_1230, resampling=Resampling.bilinear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_915.isel(time=0).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_1230.rio.resolution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_1230.isel(time=0).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_aggr.isel(time=0).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray_beam as xbeam \n",
    "import xarray\n",
    "from dataclasses import dataclass\n",
    "import apache_beam as beam\n",
    "from pangeo_forge_recipes.transforms import (\n",
    "    OpenURLWithFSSpec, OpenWithXarray, StoreToZarr, Indexed, T\n",
    ")\n",
    "\n",
    "all_days = pd.date_range('2013-01-01', '2014-01-01', freq='1D')\n",
    "\n",
    "def load_one_example(time: pd.Timestamp) -> tuple[xbeam.Key, xarray.Dataset]:\n",
    "    key = xbeam.Key({'time': (time - all_days[0]).days})\n",
    "    dataset = ds.sel(time=[time])  # replace with your code to create one example\n",
    "    return key, dataset\n",
    "\n",
    "_, example = load_one_example(all_days[0])\n",
    "template = xbeam.make_template(example).squeeze('time', drop=True).expand_dims(time=all_days)\n",
    "zarr_chunks = {'time': 100}  # desired chunking along \"time\", e.g., for more efficient storage in Zarr\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Preprocessor(beam.PTransform):\n",
    "    \"\"\"\n",
    "    Preprocessor for xarray datasets.\n",
    "    Set all data_variables except for `variable_id` attrs to coord\n",
    "    Add additional information \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def _keep_only_variable_id(item: Indexed[T]) -> Indexed[T]:\n",
    "        \"\"\"\n",
    "        Many netcdfs contain variables other than the one specified in the `variable_id` facet. \n",
    "        Set them all to coords\n",
    "        \"\"\"\n",
    "        index, ds = item\n",
    "        print(f\"Preprocessing before {ds =}\")\n",
    "        new_coords_vars = [var for var in ds.data_vars if var != ds.attrs['variable_id']]\n",
    "        ds = ds.set_coords(new_coords_vars)\n",
    "        print(f\"Preprocessing after {ds =}\")\n",
    "        return index, ds\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sanitize_attrs(item: Indexed[T]) -> Indexed[T]:\n",
    "        \"\"\"Removes non-ascii characters from attributes see https://github.com/pangeo-forge/pangeo-forge-recipes/issues/586\"\"\"\n",
    "        index, ds = item\n",
    "        for att, att_value in ds.attrs.items():\n",
    "            if isinstance(att_value, str):\n",
    "                new_value=att_value.encode(\"utf-8\", 'ignore').decode()\n",
    "                if new_value != att_value:\n",
    "                    print(f\"Sanitized datasets attributes field {att}: \\n {att_value} \\n ----> \\n {new_value}\")\n",
    "                    ds.attrs[att] = new_value\n",
    "        return index, ds\n",
    "  \n",
    "    def expand(self, pcoll: beam.PCollection) -> beam.PCollection:\n",
    "        return ( pcoll \n",
    "            | \"Fix coordinates\" >> beam.Map(self._keep_only_variable_id)\n",
    "            | \"Sanitize Attrs\" >> beam.Map(self._sanitize_attrs)\n",
    "        )\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    (\n",
    "        p\n",
    "        | beam.Create(all_days)\n",
    "        | beam.Map(load_one_example)\n",
    "        | xbeam.ConsolidateChunks(zarr_chunks)\n",
    "        | xbeam.ChunksToZarr('example-data-v4.zarr', template, zarr_chunks)\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ric_gis2_py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
