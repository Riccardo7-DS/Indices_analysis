{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Riccardo\\anaconda3\\envs\\gis2_py39\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: [WinError 127] The specified procedure could not be found\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ERA5 raster has spatial dimensions: (0.10000000419197502, -0.1000000004915847)\n",
      "NDVI dataset resolution: (0.043782177927351104, 0.04378217792735114)\n",
      "Precipitation dataset resolution (0.10000000419197502, -0.1000000004915847)\n"
     ]
    }
   ],
   "source": [
    "from p_drought_indices.analysis.DeepLearning.pipeline_gwnet import data_preparation \n",
    "from p_drought_indices.analysis.DeepLearning.pipeline_convlstm import training_lstm\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from p_drought_indices.functions.function_clns import load_config, prepare, CNN_split, interpolate_prepare\n",
    "import numpy as np\n",
    "from p_drought_indices.analysis.DeepLearning.dataset import CustomDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "CONFIG_PATH = \"../config.yaml\"\n",
    "sub_precp, ds = data_preparation(CONFIG_PATH)\n",
    "sub_precp = sub_precp.to_dataset()\n",
    "data, target = interpolate_prepare(sub_precp, ds)\n",
    "train_split = 0.8\n",
    "training_lstm(CONFIG_PATH, data, target, train_split = train_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_CHECK(data_array:xr.DataArray):\n",
    "    # Check shape and size\n",
    "    print(\"Shape:\", data_array.shape)\n",
    "    print(\"Size:\", data_array.size)\n",
    "    \n",
    "    # Check coordinates and dimensions\n",
    "    print(\"Dimensions:\", data_array.dims)\n",
    "    print(\"Coordinates:\", data_array.coords)\n",
    "    \n",
    "    # Check data type and values\n",
    "    print(\"Data Type:\", data_array.dtype)\n",
    "    print(\"Data Values:\", data_array.values)\n",
    "    \n",
    "    # Check for missing values\n",
    "    print(\"Missing Values:\", data_array.isnull().any())\n",
    "    \n",
    "    # Check attributes\n",
    "    print(\"Attributes:\", data_array.attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Riccardo\\anaconda3\\envs\\gis2_py39\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: [WinError 127] The specified procedure could not be found\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "from p_drought_indices.analysis.DeepLearning.pipeline_gwnet import data_preparation, get_dataloader, main\n",
    "from p_drought_indices.analysis.DeepLearning.pipeline_convlstm import training_lstm\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from p_drought_indices.functions.function_clns import load_config, prepare, CNN_split, interpolate_prepare, subsetting_pipeline, get_lat_lon_window\n",
    "import numpy as np\n",
    "from p_drought_indices.analysis.DeepLearning.dataset import CustomDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import xarray as xr\n",
    "\n",
    "\n",
    "CONFIG_PATH = \"../config.yaml\"\n",
    "config = load_config(CONFIG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ERA5 raster has spatial dimensions: (0.10000000419197502, -0.1000000004915847)\n",
      "NDVI dataset resolution: (0.043782177927351104, 0.04378217792735114)\n",
      "Precipitation dataset resolution (0.10000000419197502, -0.1000000004915847)\n",
      "The features have dimensions: (730, 2531, 1)\n"
     ]
    }
   ],
   "source": [
    "sub_precp, ds =  data_preparation(CONFIG_PATH, ndvi_dataset=\"smoothed_ndvi_1_old.nc\")\n",
    "x_df = sub_precp.to_dataframe()\n",
    "x_df_1 = x_df.swaplevel(1,2)\n",
    "for col in [\"spatial_ref\",\"crs\"]:\n",
    "    if col in x_df_1:\n",
    "        x_df_1.drop(columns={col}, inplace=True)\n",
    "x_df_2 = x_df_1.dropna(subset={\"tp\"})\n",
    "x_df_3 = x_df_2.sort_values([\"lat\", \"lon\",\"time\"],ascending=False)\n",
    "data_x_unstack = x_df_3.unstack([\"lat\",\"lon\"])\n",
    "#x_unstack = data_x_unstack.to_numpy()\n",
    "num_samples, num_nodes = data_x_unstack.shape\n",
    "x_unstack = np.expand_dims(data_x_unstack, axis=-1)\n",
    "print(\"The features have dimensions:\", x_unstack.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The instance have dimensions: (730, 2531, 1)\n"
     ]
    }
   ],
   "source": [
    "y_df = ds.to_dataframe()\n",
    "for col in [\"spatial_ref\",\"crs\"]:\n",
    "    if col in y_df:\n",
    "        y_df.drop(columns={col}, inplace=True)\n",
    "y_df = y_df.dropna(subset={\"ndvi\"})\n",
    "y_df = y_df.sort_values([\"lat\", \"lon\",\"time\"],ascending=False)\n",
    "y_df = y_df.reset_index().set_index([\"time\",\"lon\",\"lat\"])\n",
    "y_df = y_df[y_df.index.isin(x_df_3.index)]\n",
    "data_y_unstack = y_df.unstack([\"lat\",\"lon\"])\n",
    "y_unstack = data_y_unstack.to_numpy()\n",
    "y_unstack = np.expand_dims(y_unstack, axis=-1)\n",
    "print(\"The instance have dimensions:\", y_unstack.shape)\n",
    "\n",
    "### changes with features\n",
    "st_df = x_df_3.reset_index()[[\"lon\",\"lat\"]].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2990080\n",
      "2990080\n",
      "1877910\n",
      "1877910\n",
      "730\n"
     ]
    }
   ],
   "source": [
    "for i in [x_df, x_df_1, x_df_2, x_df_3, data_x_unstack]:\n",
    "    print(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from p_drought_indices.analysis.DeepLearning.pipeline_gwnet import generate_adj_dist\n",
    "adj_dist = generate_adj_dist(st_df)\n",
    "seq_length_x = seq_length_y = 12\n",
    "y_start = 1\n",
    "x_offsets = np.sort(np.concatenate((np.arange(-(seq_length_x - 1), 1, 1),)))\n",
    "\n",
    "with open(os.path.join(config[\"DEFAULT\"][\"data\"], \"graph_net/esa_adj_dist.pkl\"), 'wb') as f:\n",
    "        pickle.dump(adj_dist, f, protocol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ERA5 raster has spatial dimensions: (0.10000000419197502, -0.1000000004915847)\n",
      "NDVI dataset resolution: (0.043782177927351104, 0.04378217792735114)\n",
      "Precipitation dataset resolution (0.10000000419197502, -0.1000000004915847)\n",
      "The features have dimensions: (730, 2552, 1)\n"
     ]
    }
   ],
   "source": [
    "n_sub_precp, n_ds =  data_preparation(CONFIG_PATH, ndvi_dataset=\"smoothed_ndvi_1.nc\")\n",
    "n_x_df = n_sub_precp.to_dataframe()\n",
    "n_x_df_1 = n_x_df.swaplevel(1,2)\n",
    "n_x_df_2 = n_x_df_1.dropna(subset={\"tp\"}).drop(columns={\"spatial_ref\",\"crs\"})\n",
    "n_x_df_3 = n_x_df_2.sort_values([\"lat\", \"lon\",\"time\"],ascending=False)\n",
    "n_data_x_unstack = n_x_df_3.unstack([\"lat\",\"lon\"])\n",
    "#x_unstack = data_x_unstack.to_numpy()\n",
    "n_num_samples, n_num_nodes = n_data_x_unstack.shape\n",
    "n_x_unstack = np.expand_dims(n_data_x_unstack, axis=-1)\n",
    "print(\"The features have dimensions:\", n_x_unstack.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2990080\n",
      "2990080\n",
      "1861850\n",
      "1861850\n",
      "730\n"
     ]
    }
   ],
   "source": [
    "for i in [n_x_df, n_x_df_1, n_x_df_2, n_x_df_3, n_data_x_unstack]:\n",
    "    print(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The instance have dimensions: (730, 2552, 1)\n"
     ]
    }
   ],
   "source": [
    "n_y_df = n_ds.to_dataframe()\n",
    "n_y_df = n_y_df.dropna(subset={\"ndvi\"}).drop(columns={\"spatial_ref\",\"crs\"})\n",
    "n_y_df = n_y_df.sort_values([\"lat\", \"lon\",\"time\"],ascending=False)\n",
    "n_y_df = n_y_df.reset_index().set_index([\"time\",\"lon\",\"lat\"])\n",
    "n_y_df = n_y_df[n_y_df.index.isin(n_x_df_3.index)]\n",
    "n_data_y_unstack = n_y_df.unstack([\"lat\",\"lon\"])\n",
    "n_y_unstack = n_data_y_unstack.to_numpy()\n",
    "n_y_unstack = np.expand_dims(n_y_unstack, axis=-1)\n",
    "print(\"The instance have dimensions:\", n_y_unstack.shape)\n",
    "\n",
    "### changes with features\n",
    "n_st_df = n_x_df_3.reset_index()[[\"lon\",\"lat\"]].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from p_drought_indices.analysis.DeepLearning.pipeline_gwnet import generate_adj_dist\n",
    "adj_dist = generate_adj_dist(n_st_df)\n",
    "with open(os.path.join(config[\"DEFAULT\"][\"data\"], \"graph_net/n_adj_dist.pkl\"), 'wb') as f:\n",
    "        pickle.dump(adj_dist, f, protocol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape:  (707, 12, 2574, 1) , y shape:  (707, 12, 2574, 1)\n",
      "train x:  (511, 12, 2574, 1) y: (511, 12, 2574, 1)\n",
      "val x:  (73, 12, 2574, 1) y: (73, 12, 2574, 1)\n",
      "test x:  (146, 12, 2574, 1) y: (146, 12, 2574, 1)\n"
     ]
    }
   ],
   "source": [
    "from p_drought_indices.analysis.DeepLearning.pipeline_gwnet import load_dataset, MetricsRecorder\n",
    "\n",
    "output_dir = os.path.join(config[\"DEFAULT\"][\"data\"],  \"graph_net\")\n",
    "seq_length_x = seq_length_y = 12\n",
    "y_start = 1\n",
    "x_offsets = np.sort(np.concatenate((np.arange(-(seq_length_x - 1), 1, 1),)))\n",
    "\n",
    "# Predict the next one hour\n",
    "y_offsets = np.sort(np.arange(y_start, (seq_length_y + 1), 1))\n",
    "x, y = [], []\n",
    "min_t = abs(min(x_offsets))\n",
    "max_t = abs(num_samples - abs(max(y_offsets)))  # Exclusive\n",
    "\n",
    "for t in range(min_t, max_t):  # t is the index of the last observation.\n",
    "    x.append(x_unstack[t + x_offsets, ...])\n",
    "    y.append(y_unstack[t + y_offsets, ...])\n",
    "x = np.stack(x, axis=0)\n",
    "y = np.stack(y, axis=0)\n",
    "print(\"x shape: \", x.shape, \", y shape: \", y.shape)\n",
    "num_test = round(num_samples * 0.2)\n",
    "num_train = round(num_samples * 0.7)\n",
    "num_val = num_samples - num_test - num_train\n",
    "x_train, y_train = x[:num_train], y[:num_train]\n",
    "x_val, y_val = (\n",
    "    x[num_train: num_train + num_val],\n",
    "    y[num_train: num_train + num_val],\n",
    ")\n",
    "x_test, y_test = x[-num_test:], y[-num_test:]\n",
    "for cat in [\"train\", \"val\", \"test\"]:\n",
    "    _x, _y = locals()[\"x_\" + cat], locals()[\"y_\" + cat]\n",
    "    print(cat, \"x: \", _x.shape, \"y:\", _y.shape)\n",
    "    np.savez_compressed(\n",
    "        os.path.join(output_dir, f\"{cat}.npz\"),\n",
    "        x=_x,\n",
    "        y=_y,\n",
    "        x_offsets=x_offsets.reshape(list(x_offsets.shape) + [1]),\n",
    "        y_offsets=y_offsets.reshape(list(y_offsets.shape) + [1]),\n",
    "    )\n",
    "batch_size = config[\"GWNET\"][\"batch_size\"]\n",
    "dataloader = load_dataset(output_dir, batch_size, batch_size, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training...\n",
      "Gradient Stats: nodevec1 nan nan nan\n",
      "Gradient Stats: nodevec2 nan nan nan\n",
      "Gradient Stats: filter_convs.0.weight nan nan nan\n",
      "Gradient Stats: filter_convs.0.bias nan nan nan\n",
      "Gradient Stats: filter_convs.1.weight nan nan nan\n",
      "Gradient Stats: filter_convs.1.bias nan nan nan\n",
      "Gradient Stats: filter_convs.2.weight nan nan nan\n",
      "Gradient Stats: filter_convs.2.bias nan nan nan\n",
      "Gradient Stats: filter_convs.3.weight nan nan nan\n",
      "Gradient Stats: filter_convs.3.bias nan nan nan\n",
      "Gradient Stats: filter_convs.4.weight nan nan nan\n",
      "Gradient Stats: filter_convs.4.bias nan nan nan\n",
      "Gradient Stats: filter_convs.5.weight nan nan nan\n",
      "Gradient Stats: filter_convs.5.bias nan nan nan\n",
      "Gradient Stats: filter_convs.6.weight nan nan nan\n",
      "Gradient Stats: filter_convs.6.bias nan nan nan\n",
      "Gradient Stats: filter_convs.7.weight nan nan nan\n",
      "Gradient Stats: filter_convs.7.bias nan nan nan\n",
      "Gradient Stats: gate_convs.0.weight nan nan nan\n",
      "Gradient Stats: gate_convs.0.bias nan nan nan\n",
      "Gradient Stats: gate_convs.1.weight nan nan nan\n",
      "Gradient Stats: gate_convs.1.bias nan nan nan\n",
      "Gradient Stats: gate_convs.2.weight nan nan nan\n",
      "Gradient Stats: gate_convs.2.bias nan nan nan\n",
      "Gradient Stats: gate_convs.3.weight nan nan nan\n",
      "Gradient Stats: gate_convs.3.bias nan nan nan\n",
      "Gradient Stats: gate_convs.4.weight nan nan nan\n",
      "Gradient Stats: gate_convs.4.bias nan nan nan\n",
      "Gradient Stats: gate_convs.5.weight nan nan nan\n",
      "Gradient Stats: gate_convs.5.bias nan nan nan\n",
      "Gradient Stats: gate_convs.6.weight nan nan nan\n",
      "Gradient Stats: gate_convs.6.bias nan nan nan\n",
      "Gradient Stats: gate_convs.7.weight nan nan nan\n",
      "Gradient Stats: gate_convs.7.bias nan nan nan\n",
      "Gradient Stats: skip_convs.0.weight nan nan nan\n",
      "Gradient Stats: skip_convs.0.bias nan nan nan\n",
      "Gradient Stats: skip_convs.1.weight nan nan nan\n",
      "Gradient Stats: skip_convs.1.bias nan nan nan\n",
      "Gradient Stats: skip_convs.2.weight nan nan nan\n",
      "Gradient Stats: skip_convs.2.bias nan nan nan\n",
      "Gradient Stats: skip_convs.3.weight nan nan nan\n",
      "Gradient Stats: skip_convs.3.bias nan nan nan\n",
      "Gradient Stats: skip_convs.4.weight nan nan nan\n",
      "Gradient Stats: skip_convs.4.bias nan nan nan\n",
      "Gradient Stats: skip_convs.5.weight nan nan nan\n",
      "Gradient Stats: skip_convs.5.bias nan nan nan\n",
      "Gradient Stats: skip_convs.6.weight nan nan nan\n",
      "Gradient Stats: skip_convs.6.bias nan nan nan\n",
      "Gradient Stats: skip_convs.7.weight nan nan nan\n",
      "Gradient Stats: skip_convs.7.bias nan nan nan\n",
      "Gradient Stats: bn.0.weight nan nan nan\n",
      "Gradient Stats: bn.0.bias nan nan nan\n",
      "Gradient Stats: bn.1.weight nan nan nan\n",
      "Gradient Stats: bn.1.bias nan nan nan\n",
      "Gradient Stats: bn.2.weight nan nan nan\n",
      "Gradient Stats: bn.2.bias nan nan nan\n",
      "Gradient Stats: bn.3.weight nan nan nan\n",
      "Gradient Stats: bn.3.bias nan nan nan\n",
      "Gradient Stats: bn.4.weight nan nan nan\n",
      "Gradient Stats: bn.4.bias nan nan nan\n",
      "Gradient Stats: bn.5.weight nan nan nan\n",
      "Gradient Stats: bn.5.bias nan nan nan\n",
      "Gradient Stats: bn.6.weight nan nan nan\n",
      "Gradient Stats: bn.6.bias nan nan nan\n",
      "Gradient Stats: gconv.0.mlp.mlp.weight nan nan nan\n",
      "Gradient Stats: gconv.0.mlp.mlp.bias nan nan nan\n",
      "Gradient Stats: gconv.1.mlp.mlp.weight nan nan nan\n",
      "Gradient Stats: gconv.1.mlp.mlp.bias nan nan nan\n",
      "Gradient Stats: gconv.2.mlp.mlp.weight nan nan nan\n",
      "Gradient Stats: gconv.2.mlp.mlp.bias nan nan nan\n",
      "Gradient Stats: gconv.3.mlp.mlp.weight nan nan nan\n",
      "Gradient Stats: gconv.3.mlp.mlp.bias nan nan nan\n",
      "Gradient Stats: gconv.4.mlp.mlp.weight nan nan nan\n",
      "Gradient Stats: gconv.4.mlp.mlp.bias nan nan nan\n",
      "Gradient Stats: gconv.5.mlp.mlp.weight nan nan nan\n",
      "Gradient Stats: gconv.5.mlp.mlp.bias nan nan nan\n",
      "Gradient Stats: gconv.6.mlp.mlp.weight nan nan nan\n",
      "Gradient Stats: gconv.6.mlp.mlp.bias nan nan nan\n",
      "Gradient Stats: start_conv.weight nan nan nan\n",
      "Gradient Stats: start_conv.bias nan nan nan\n",
      "Gradient Stats: end_conv_1.weight nan nan nan\n",
      "Gradient Stats: end_conv_1.bias nan nan nan\n",
      "Gradient Stats: end_conv_2.weight nan nan nan\n",
      "Gradient Stats: end_conv_2.bias nan nan nan\n",
      "Iter: 000, Train Loss: 0.0000, Train MAPE: 0.0000, Train RMSE: 0.0000\n",
      "Gradient Stats: nodevec1 nan nan nan\n",
      "Gradient Stats: nodevec2 nan nan nan\n",
      "Gradient Stats: filter_convs.0.weight nan nan nan\n",
      "Gradient Stats: filter_convs.0.bias nan nan nan\n",
      "Gradient Stats: filter_convs.1.weight nan nan nan\n",
      "Gradient Stats: filter_convs.1.bias nan nan nan\n",
      "Gradient Stats: filter_convs.2.weight nan nan nan\n",
      "Gradient Stats: filter_convs.2.bias nan nan nan\n",
      "Gradient Stats: filter_convs.3.weight nan nan nan\n",
      "Gradient Stats: filter_convs.3.bias nan nan nan\n",
      "Gradient Stats: filter_convs.4.weight nan nan nan\n",
      "Gradient Stats: filter_convs.4.bias nan nan nan\n",
      "Gradient Stats: filter_convs.5.weight nan nan nan\n",
      "Gradient Stats: filter_convs.5.bias nan nan nan\n",
      "Gradient Stats: filter_convs.6.weight nan nan nan\n",
      "Gradient Stats: filter_convs.6.bias nan nan nan\n",
      "Gradient Stats: filter_convs.7.weight nan nan nan\n",
      "Gradient Stats: filter_convs.7.bias nan nan nan\n",
      "Gradient Stats: gate_convs.0.weight nan nan nan\n",
      "Gradient Stats: gate_convs.0.bias nan nan nan\n",
      "Gradient Stats: gate_convs.1.weight nan nan nan\n",
      "Gradient Stats: gate_convs.1.bias nan nan nan\n",
      "Gradient Stats: gate_convs.2.weight nan nan nan\n",
      "Gradient Stats: gate_convs.2.bias nan nan nan\n",
      "Gradient Stats: gate_convs.3.weight nan nan nan\n",
      "Gradient Stats: gate_convs.3.bias nan nan nan\n",
      "Gradient Stats: gate_convs.4.weight nan nan nan\n",
      "Gradient Stats: gate_convs.4.bias nan nan nan\n",
      "Gradient Stats: gate_convs.5.weight nan nan nan\n",
      "Gradient Stats: gate_convs.5.bias nan nan nan\n",
      "Gradient Stats: gate_convs.6.weight nan nan nan\n",
      "Gradient Stats: gate_convs.6.bias nan nan nan\n",
      "Gradient Stats: gate_convs.7.weight nan nan nan\n",
      "Gradient Stats: gate_convs.7.bias nan nan nan\n",
      "Gradient Stats: skip_convs.0.weight nan nan nan\n",
      "Gradient Stats: skip_convs.0.bias nan nan nan\n",
      "Gradient Stats: skip_convs.1.weight nan nan nan\n",
      "Gradient Stats: skip_convs.1.bias nan nan nan\n",
      "Gradient Stats: skip_convs.2.weight nan nan nan\n",
      "Gradient Stats: skip_convs.2.bias nan nan nan\n",
      "Gradient Stats: skip_convs.3.weight nan nan nan\n",
      "Gradient Stats: skip_convs.3.bias nan nan nan\n",
      "Gradient Stats: skip_convs.4.weight nan nan nan\n",
      "Gradient Stats: skip_convs.4.bias nan nan nan\n",
      "Gradient Stats: skip_convs.5.weight nan nan nan\n",
      "Gradient Stats: skip_convs.5.bias nan nan nan\n",
      "Gradient Stats: skip_convs.6.weight nan nan nan\n",
      "Gradient Stats: skip_convs.6.bias nan nan nan\n",
      "Gradient Stats: skip_convs.7.weight nan nan nan\n",
      "Gradient Stats: skip_convs.7.bias nan nan nan\n",
      "Gradient Stats: bn.0.weight nan nan nan\n",
      "Gradient Stats: bn.0.bias nan nan nan\n",
      "Gradient Stats: bn.1.weight nan nan nan\n",
      "Gradient Stats: bn.1.bias nan nan nan\n",
      "Gradient Stats: bn.2.weight nan nan nan\n",
      "Gradient Stats: bn.2.bias nan nan nan\n",
      "Gradient Stats: bn.3.weight nan nan nan\n",
      "Gradient Stats: bn.3.bias nan nan nan\n",
      "Gradient Stats: bn.4.weight nan nan nan\n",
      "Gradient Stats: bn.4.bias nan nan nan\n",
      "Gradient Stats: bn.5.weight nan nan nan\n",
      "Gradient Stats: bn.5.bias nan nan nan\n",
      "Gradient Stats: bn.6.weight nan nan nan\n",
      "Gradient Stats: bn.6.bias nan nan nan\n",
      "Gradient Stats: gconv.0.mlp.mlp.weight nan nan nan\n",
      "Gradient Stats: gconv.0.mlp.mlp.bias nan nan nan\n",
      "Gradient Stats: gconv.1.mlp.mlp.weight nan nan nan\n",
      "Gradient Stats: gconv.1.mlp.mlp.bias nan nan nan\n",
      "Gradient Stats: gconv.2.mlp.mlp.weight nan nan nan\n",
      "Gradient Stats: gconv.2.mlp.mlp.bias nan nan nan\n",
      "Gradient Stats: gconv.3.mlp.mlp.weight nan nan nan\n",
      "Gradient Stats: gconv.3.mlp.mlp.bias nan nan nan\n",
      "Gradient Stats: gconv.4.mlp.mlp.weight nan nan nan\n",
      "Gradient Stats: gconv.4.mlp.mlp.bias nan nan nan\n",
      "Gradient Stats: gconv.5.mlp.mlp.weight nan nan nan\n",
      "Gradient Stats: gconv.5.mlp.mlp.bias nan nan nan\n",
      "Gradient Stats: gconv.6.mlp.mlp.weight nan nan nan\n",
      "Gradient Stats: gconv.6.mlp.mlp.bias nan nan nan\n",
      "Gradient Stats: start_conv.weight nan nan nan\n",
      "Gradient Stats: start_conv.bias nan nan nan\n",
      "Gradient Stats: end_conv_1.weight nan nan nan\n",
      "Gradient Stats: end_conv_1.bias nan nan nan\n",
      "Gradient Stats: end_conv_2.weight nan nan nan\n",
      "Gradient Stats: end_conv_2.bias nan nan nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 41\u001b[0m\n\u001b[0;32m     39\u001b[0m trainy \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mTensor(y)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     40\u001b[0m trainy \u001b[39m=\u001b[39m trainy\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m)\n\u001b[1;32m---> 41\u001b[0m metrics \u001b[39m=\u001b[39m engine\u001b[39m.\u001b[39;49mtrain(trainx, trainy[:,\u001b[39m0\u001b[39;49m,:,:])\n\u001b[0;32m     42\u001b[0m train_loss\u001b[39m.\u001b[39mappend(metrics[\u001b[39m0\u001b[39m])\n\u001b[0;32m     43\u001b[0m train_mape\u001b[39m.\u001b[39mappend(metrics[\u001b[39m1\u001b[39m])\n",
      "File \u001b[1;32mc:\\users\\riccardo\\desktop\\other\\package\\indices_analysis\\p_drought_indices\\analysis\\DeepLearning\\pipeline_gwnet.py:404\u001b[0m, in \u001b[0;36mtrainer.train\u001b[1;34m(self, input, real_val)\u001b[0m\n\u001b[0;32m    402\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m    403\u001b[0m \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m,(\u001b[39m1\u001b[39m,\u001b[39m0\u001b[39m,\u001b[39m0\u001b[39m,\u001b[39m0\u001b[39m))\n\u001b[1;32m--> 404\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    405\u001b[0m output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m,\u001b[39m3\u001b[39m)\n\u001b[0;32m    406\u001b[0m \u001b[39m#output = [batch_size,12,num_nodes,1]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Riccardo\\anaconda3\\envs\\gis2_py39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\users\\riccardo\\desktop\\other\\package\\indices_analysis\\p_drought_indices\\analysis\\DeepLearning\\gwnet.py:192\u001b[0m, in \u001b[0;36mgwnet.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgcn_bool \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msupports \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    191\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maddaptadj:\n\u001b[1;32m--> 192\u001b[0m         x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgconv[i](x, new_supports)\n\u001b[0;32m    193\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    194\u001b[0m         x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgconv[i](x,\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msupports)\n",
      "File \u001b[1;32mc:\\Users\\Riccardo\\anaconda3\\envs\\gis2_py39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\users\\riccardo\\desktop\\other\\package\\indices_analysis\\p_drought_indices\\analysis\\DeepLearning\\gwnet.py:39\u001b[0m, in \u001b[0;36mgcn.forward\u001b[1;34m(self, x, support)\u001b[0m\n\u001b[0;32m     37\u001b[0m out\u001b[39m.\u001b[39mappend(x1)\n\u001b[0;32m     38\u001b[0m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m2\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39morder \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m---> 39\u001b[0m     x2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnconv(x1,a)\n\u001b[0;32m     40\u001b[0m     out\u001b[39m.\u001b[39mappend(x2)\n\u001b[0;32m     41\u001b[0m     x1 \u001b[39m=\u001b[39m x2\n",
      "File \u001b[1;32mc:\\Users\\Riccardo\\anaconda3\\envs\\gis2_py39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\users\\riccardo\\desktop\\other\\package\\indices_analysis\\p_drought_indices\\analysis\\DeepLearning\\gwnet.py:13\u001b[0m, in \u001b[0;36mnconv.forward\u001b[1;34m(self, x, A)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m,x, A):\n\u001b[1;32m---> 13\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49meinsum(\u001b[39m'\u001b[39;49m\u001b[39mncvl,vw->ncwl\u001b[39;49m\u001b[39m'\u001b[39;49m,(x,A))\n\u001b[0;32m     14\u001b[0m     \u001b[39mreturn\u001b[39;00m x\u001b[39m.\u001b[39mcontiguous()\n",
      "File \u001b[1;32mc:\\Users\\Riccardo\\anaconda3\\envs\\gis2_py39\\lib\\site-packages\\torch\\functional.py:373\u001b[0m, in \u001b[0;36meinsum\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    370\u001b[0m     _operands \u001b[39m=\u001b[39m operands[\u001b[39m0\u001b[39m]\n\u001b[0;32m    371\u001b[0m     \u001b[39m# recurse incase operands contains value that has torch function\u001b[39;00m\n\u001b[0;32m    372\u001b[0m     \u001b[39m# in the original implementation this line is omitted\u001b[39;00m\n\u001b[1;32m--> 373\u001b[0m     \u001b[39mreturn\u001b[39;00m einsum(equation, \u001b[39m*\u001b[39;49m_operands)\n\u001b[0;32m    375\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(operands) \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m2\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m opt_einsum\u001b[39m.\u001b[39menabled:\n\u001b[0;32m    376\u001b[0m     \u001b[39m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[0;32m    377\u001b[0m     \u001b[39m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[0;32m    378\u001b[0m     \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39meinsum(equation, operands)  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Riccardo\\anaconda3\\envs\\gis2_py39\\lib\\site-packages\\torch\\functional.py:378\u001b[0m, in \u001b[0;36meinsum\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    373\u001b[0m     \u001b[39mreturn\u001b[39;00m einsum(equation, \u001b[39m*\u001b[39m_operands)\n\u001b[0;32m    375\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(operands) \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m2\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m opt_einsum\u001b[39m.\u001b[39menabled:\n\u001b[0;32m    376\u001b[0m     \u001b[39m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[0;32m    377\u001b[0m     \u001b[39m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[1;32m--> 378\u001b[0m     \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39;49meinsum(equation, operands)  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    380\u001b[0m path \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    381\u001b[0m \u001b[39mif\u001b[39;00m opt_einsum\u001b[39m.\u001b[39mis_available():\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import time\n",
    "from p_drought_indices.analysis.DeepLearning.pipeline_gwnet import save_figures, trainer, load_adj\n",
    "device = \"cpu\" #torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "seq_length=12\n",
    "nhid=32\n",
    "in_dim =1\n",
    "adjtype = \"doubletransition\"\n",
    "learning_rate=0.001\n",
    "dropout=0.3\n",
    "weight_decay=0.0001\n",
    "gcn_bool = \"store_true\"\n",
    "addaptadj = \"store_true\"\n",
    "print_every = 50\n",
    "epochs = 50\n",
    "\n",
    "adj_path = os.path.join(config[\"DEFAULT\"][\"data\"], \"graph_net/esa_adj_dist.pkl\")\n",
    "adj_mx = load_adj(adj_path,  adjtype)\n",
    "scaler = dataloader['scaler']\n",
    "supports = [torch.tensor(i).to(device) for i in adj_mx]\n",
    "metrics_recorder = MetricsRecorder()\n",
    "adjinit = supports[0]\n",
    "engine = trainer(scaler, in_dim, seq_length, num_nodes, nhid, dropout,\n",
    "                     learning_rate, weight_decay, device, supports, gcn_bool, addaptadj,\n",
    "                     adjinit)\n",
    "print(\"start training...\",flush=True)\n",
    "his_loss =[]\n",
    "val_time = []\n",
    "train_time = []\n",
    "for i in range(1,epochs+1):\n",
    "    train_loss = []\n",
    "    train_mape = []\n",
    "    train_rmse = []\n",
    "    t1 = time.time()\n",
    "    dataloader['train_loader'].shuffle()\n",
    "    for iter, (x, y) in enumerate(dataloader['train_loader'].get_iterator()):\n",
    "        trainx = torch.Tensor(x).to(device)\n",
    "        trainx= trainx.transpose(1, 3)\n",
    "        trainy = torch.Tensor(y).to(device)\n",
    "        trainy = trainy.transpose(1, 3)\n",
    "        metrics = engine.train(trainx, trainy[:,0,:,:])\n",
    "        train_loss.append(metrics[0])\n",
    "        train_mape.append(metrics[1])\n",
    "        train_rmse.append(metrics[2])\n",
    "        if iter % print_every == 0 :\n",
    "            log = 'Iter: {:03d}, Train Loss: {:.4f}, Train MAPE: {:.4f}, Train RMSE: {:.4f}'\n",
    "            print(log.format(iter, train_loss[-1], train_mape[-1], train_rmse[-1]),flush=True)\n",
    "    t2 = time.time()\n",
    "    train_time.append(t2-t1)\n",
    "    #validation\n",
    "    valid_loss = []\n",
    "    valid_mape = []\n",
    "    valid_rmse = []\n",
    "    s1 = time.time()\n",
    "    for iter, (x, y) in enumerate(dataloader['val_loader'].get_iterator()):\n",
    "        testx = torch.Tensor(x).to(device)\n",
    "        testx = testx.transpose(1, 3)\n",
    "        testy = torch.Tensor(y).to(device)\n",
    "        testy = testy.transpose(1, 3)\n",
    "        metrics = engine.eval(testx, testy[:,0,:,:])\n",
    "        valid_loss.append(metrics[0])\n",
    "        valid_mape.append(metrics[1])\n",
    "        valid_rmse.append(metrics[2])\n",
    "    \n",
    "    s2 = time.time()\n",
    "    log = 'Epoch: {:03d}, Inference Time: {:.4f} secs'\n",
    "    print(log.format(i,(s2-s1)))\n",
    "    val_time.append(s2-s1)\n",
    "    mtrain_loss = np.mean(train_loss)\n",
    "    mtrain_mape = np.mean(train_mape)\n",
    "    mtrain_rmse = np.mean(train_rmse)\n",
    "    metrics_recorder.add_train_metrics(mtrain_mape, mtrain_rmse, mtrain_loss)\n",
    "    mvalid_loss = np.mean(valid_loss)\n",
    "    mvalid_mape = np.mean(valid_mape)\n",
    "    mvalid_rmse = np.mean(valid_rmse)\n",
    "    his_loss.append(mvalid_loss)\n",
    "    metrics_recorder.add_val_metrics(mvalid_mape, mvalid_rmse, mvalid_loss)\n",
    "    save_figures(config=config, epoch=i, train_loss=metrics_recorder.train_loss, \n",
    "                train_mape=metrics_recorder.train_mape, train_rmse=metrics_recorder.train_rmse, \n",
    "                test_loss=metrics_recorder.val_loss, test_rmse=metrics_recorder.val_loss, \n",
    "                test_mape=metrics_recorder.val_mape)\n",
    "    log = 'Epoch: {:03d}, Train Loss: {:.4f}, Train MAPE: {:.4f}, Train RMSE: {:.4f}, Valid Loss: {:.4f}, Valid MAPE: {:.4f}, Valid RMSE: {:.4f}, Training Time: {:.4f}/epoch'\n",
    "    print(log.format(i, mtrain_loss, mtrain_mape, mtrain_rmse, mvalid_loss, mvalid_mape, mvalid_rmse, (t2 - t1)),flush=True)\n",
    "    #torch.save(engine.model.state_dict(), args.save+\"_epoch_\"+str(i)+\"_\"+str(round(mvalid_loss,2))+\".pth\")\n",
    "print(\"Average Training Time: {:.4f} secs/epoch\".format(np.mean(train_time)))\n",
    "print(\"Average Inference Time: {:.4f} secs\".format(np.mean(val_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gis_py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
