{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ERA5 raster has spatial dimensions: (0.10000000419197502, -0.1000000004915847)\n",
      "NDVI dataset resolution: (0.043782177927351104, 0.04378217792735114)\n",
      "Precipitation dataset resolution (0.10000000419197502, -0.1000000004915847)\n",
      "The features have dimensions: (730, 92, 1)\n",
      "The instance have dimensions: (730, 92, 1)\n",
      "x shape:  (707, 12, 92, 1) , y shape:  (707, 12, 92, 1)\n",
      "train x:  (511, 12, 92, 1) y: (511, 12, 92, 1)\n",
      "val x:  (73, 12, 92, 1) y: (73, 12, 92, 1)\n",
      "test x:  (146, 12, 92, 1) y: (146, 12, 92, 1)\n"
     ]
    }
   ],
   "source": [
    "CONFIG_PATH = \"../config.yaml\"\n",
    "from p_drought_indices.analysis.DeepLearning.pipeline_gwnet import data_preparation\n",
    "dataloader, num_nodes, x_df, y_df = data_preparation(CONFIG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df = x_df.unstack([\"lat\",\"lon\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "time_ind = (x_df.index.values - x_df.index.values.astype(\"datetime64[D]\")) / np.timedelta64(1, \"D\")\n",
    "time_in_day = np.tile(time_ind, [1, num_nodes, 1]).transpose((2, 1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from p_drought_indices.functions.function_clns import load_config\n",
    "from p_drought_indices.analysis.DeepLearning.pipeline_gwnet import asym_adj\n",
    "import pickle\n",
    "\n",
    "CONFIG_PATH = \"../config.yaml\"\n",
    "config = load_config(CONFIG_PATH)\n",
    "adj_type = 'doubletransition'\n",
    "\n",
    "#adj_path = os.path.join(config[\"DEFAULT\"][\"data\"], \"graph_net/adj_dist.pkl\")\n",
    "#sensor_ids, sensor_id_to_ind, adj_mx = load_adj(adj_path,  adj_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pivoting\n",
      "Resampling\n",
      "Threshold of null rows per column 1086.2499999999998\n",
      "Columns to be removed 11\n",
      "Null values remaining 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "resample_time=\"5min\"\n",
    "output_column_name=\"bikes_available\"\n",
    "status_df_filename = config[\"DEFAULT\"][\"data\"] + \"\\status.csv\\status.csv\"\n",
    "\n",
    "#def generate_train_val_test(status_df_filename, resample_time=\"5min\", output_column_name=\"bikes_available\"):\n",
    "df = pd.read_csv(status_df_filename)\n",
    "df.time = pd.to_datetime(df.time).dt.round('min')\n",
    "print(\"Pivoting\")\n",
    "df_m = df.pivot_table(index='time', columns='station_id', values=output_column_name, aggfunc=np.min)\n",
    "print(\"Resampling\")\n",
    "df_mr = df_m.resample(resample_time).mean()\n",
    "#null treatment\n",
    "null_quantile = df_mr.isnull().sum().quantile(0.85)\n",
    "threshold_null = len(df_mr.index) - null_quantile\n",
    "print('Threshold of null rows per column', null_quantile)\n",
    "print('Columns to be removed', (df_mr.isnull().sum() > null_quantile).sum())\n",
    "df_mrn = df_mr.dropna(thresh=threshold_null, axis='columns').interpolate()\n",
    "print('Null values remaining', df_mrn.isnull().sum().sum())\n",
    "# 0 is the latest observed sample.\n",
    "x_offsets = np.sort(\n",
    "    np.concatenate((np.arange(-11, 1, 1),))\n",
    ")\n",
    "# Predict the next one hour\n",
    "y_offsets = np.sort(np.arange(1, 13, 1))\n",
    "# x: (num_samples, input_length, num_nodes, input_dim)\n",
    "# y: (num_samples, output_length, num_nodes, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape:  (210936, 12, 59, 2) , y shape:  (210936, 12, 59, 2)\n",
      "train x:  (147671, 12, 59, 2) y: (147671, 12, 59, 2)\n",
      "val x:  (21096, 12, 59, 2) y: (21096, 12, 59, 2)\n",
      "test x:  (42192, 12, 59, 2) y: (42192, 12, 59, 2)\n"
     ]
    }
   ],
   "source": [
    "df = df_mrn.copy()\n",
    "\n",
    "add_time_in_day = True\n",
    "add_day_in_week = False\n",
    "\n",
    "num_samples, num_nodes = df.shape\n",
    "data = np.expand_dims(df.values, axis=-1)\n",
    "data_list = [data]\n",
    "if add_time_in_day:\n",
    "    time_ind = (df.index.values - df.index.values.astype(\"datetime64[D]\")) / np.timedelta64(1, \"D\")\n",
    "    time_in_day = np.tile(time_ind, [1, num_nodes, 1]).transpose((2, 1, 0))\n",
    "    data_list.append(time_in_day)\n",
    "if add_day_in_week:\n",
    "    day_in_week = np.zeros(shape=(num_samples, num_nodes, 7))\n",
    "    day_in_week[np.arange(num_samples), :, df.index.dayofweek] = 1\n",
    "    data_list.append(day_in_week)\n",
    "data = np.concatenate(data_list, axis=-1)\n",
    "# epoch_len = num_samples + min(x_offsets) - max(y_offsets)\n",
    "x, y = [], []\n",
    "# t is the index of the last observation.\n",
    "min_t = abs(min(x_offsets))\n",
    "max_t = abs(num_samples - abs(max(y_offsets)))  # Exclusive\n",
    "for t in range(min_t, max_t):\n",
    "    x_t = data[t + x_offsets, ...]\n",
    "    y_t = data[t + y_offsets, ...]\n",
    "    x.append(x_t)\n",
    "    y.append(y_t)\n",
    "x = np.stack(x, axis=0)\n",
    "y = np.stack(y, axis=0)\n",
    "\n",
    "print(\"x shape: \", x.shape, \", y shape: \", y.shape)\n",
    "\n",
    "output_dir = os.path.join(config[\"DEFAULT\"][\"data\"],  \"graph_net\")\n",
    "\n",
    "from p_drought_indices.analysis.DeepLearning.pipeline_gwnet import load_dataset\n",
    "\n",
    "num_test = round(num_samples * 0.2)\n",
    "num_train = round(num_samples * 0.7)\n",
    "num_val = num_samples - num_test - num_train\n",
    "x_train, y_train = x[:num_train], y[:num_train]\n",
    "x_val, y_val = (\n",
    "    x[num_train: num_train + num_val],\n",
    "    y[num_train: num_train + num_val],\n",
    ")\n",
    "x_test, y_test = x[-num_test:], y[-num_test:]\n",
    "for cat in [\"train\", \"val\", \"test\"]:\n",
    "    _x, _y = locals()[\"x_\" + cat], locals()[\"y_\" + cat]\n",
    "    print(cat, \"x: \", _x.shape, \"y:\", _y.shape)\n",
    "    np.savez_compressed(\n",
    "        os.path.join(output_dir, f\"{cat}.npz\"),\n",
    "        x=_x,\n",
    "        y=_y,\n",
    "        x_offsets=x_offsets.reshape(list(x_offsets.shape) + [1]),\n",
    "        y_offsets=y_offsets.reshape(list(y_offsets.shape) + [1]),\n",
    "    )\n",
    "batch_size = 64\n",
    "dataloader = load_dataset(output_dir, batch_size, batch_size, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gis_py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
