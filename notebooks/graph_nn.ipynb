{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Riccardo\\anaconda3\\envs\\gis2_py39\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: [WinError 127] The specified procedure could not be found\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ERA5 raster has spatial dimensions: (0.10000000419197502, -0.1000000004915847)\n",
      "NDVI dataset resolution: (0.043782177927351104, 0.04378217792735114)\n",
      "Precipitation dataset resolution (0.10000000419197502, -0.1000000004915847)\n"
     ]
    }
   ],
   "source": [
    "from p_drought_indices.analysis.DeepLearning.pipeline_gwnet import data_preparation \n",
    "from p_drought_indices.analysis.DeepLearning.pipeline_convlstm import training_lstm\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from p_drought_indices.functions.function_clns import load_config, prepare, CNN_split, interpolate_prepare\n",
    "import numpy as np\n",
    "from p_drought_indices.analysis.DeepLearning.dataset import CustomDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "CONFIG_PATH = \"../config.yaml\"\n",
    "sub_precp, ds = data_preparation(CONFIG_PATH)\n",
    "sub_precp = sub_precp.to_dataset()\n",
    "data, target = interpolate_prepare(sub_precp, ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Riccardo\\Desktop\\Other\\package\\Indices_analysis\\notebooks\\.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot directly convert an xarray.Dataset into a numpy array. Instead, create an xarray.DataArray first, either with indexing on the Dataset or by invoking the `to_array()` method.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m train_split \u001b[39m=\u001b[39m \u001b[39m0.8\u001b[39m\n\u001b[1;32m----> 2\u001b[0m training_lstm(CONFIG_PATH, sub_precp, ds, train_split \u001b[39m=\u001b[39;49m train_split)\n",
      "File \u001b[1;32mc:\\users\\riccardo\\desktop\\other\\package\\indices_analysis\\p_drought_indices\\analysis\\DeepLearning\\pipeline_convlstm.py:69\u001b[0m, in \u001b[0;36mtraining_lstm\u001b[1;34m(CONFIG_PATH, sub_precp, ds, train_split)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mp_drought_indices\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctions\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunction_clns\u001b[39;00m \u001b[39mimport\u001b[39;00m load_config\n\u001b[0;32m     67\u001b[0m \u001b[39m#### training parameters\u001b[39;00m\n\u001b[1;32m---> 69\u001b[0m data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49marray(sub_precp) \n\u001b[0;32m     70\u001b[0m target \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(ds)\n\u001b[0;32m     71\u001b[0m train_data, test_data, train_label, test_label \u001b[39m=\u001b[39m CNN_split(data, target, \n\u001b[0;32m     72\u001b[0m                                                            split_percentage\u001b[39m=\u001b[39mtrain_split)\n",
      "File \u001b[1;32mc:\\Users\\Riccardo\\anaconda3\\envs\\gis2_py39\\lib\\site-packages\\xarray\\core\\dataset.py:1400\u001b[0m, in \u001b[0;36mDataset.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   1399\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__array__\u001b[39m(\u001b[39mself\u001b[39m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m-> 1400\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m   1401\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcannot directly convert an xarray.Dataset into a \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1402\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mnumpy array. Instead, create an xarray.DataArray \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1403\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mfirst, either with indexing on the Dataset or by \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1404\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39minvoking the `to_array()` method.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1405\u001b[0m     )\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot directly convert an xarray.Dataset into a numpy array. Instead, create an xarray.DataArray first, either with indexing on the Dataset or by invoking the `to_array()` method."
     ]
    }
   ],
   "source": [
    "train_split = 0.8\n",
    "training_lstm(CONFIG_PATH, data, target, train_split = train_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input data has shape: (1, 730, 1, 64, 64)\n",
      "The input data has shape: (1, 730, 1, 64, 64)\n",
      "torch.Size([1, 584, 1, 64, 64]) torch.Size([1, 584, 1, 64, 64]) tensor(309.7275) tensor(0.)\n",
      "torch.Size([1, 146, 1, 64, 64]) torch.Size([1, 146, 1, 64, 64]) tensor(252.2191) tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "train_split = 0.8\n",
    "batch_size = 32\n",
    "\n",
    "train_data, test_data, train_label, test_label = CNN_split(data=data, target=target, split_percentage=train_split)\n",
    "\n",
    "# create a CustomDataset object using the reshaped input data\n",
    "train_dataset = CustomDataset(train_data, train_label)\n",
    "test_dataset = CustomDataset(test_data, test_label)\n",
    "\n",
    "# create a DataLoader object that uses the dataset\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "### check shape of data\n",
    "for batch_idx, (inputs, targets) in enumerate(train_dataloader):\n",
    "    inputs = inputs.float()\n",
    "    targets = targets.float()\n",
    "    print(inputs.shape, targets.shape, inputs.max(), inputs.min())\n",
    "\n",
    "\n",
    "for batch_idx, (inputs, targets) in enumerate(test_dataloader):\n",
    "    inputs = inputs.float()\n",
    "    targets = targets.float()\n",
    "    print(inputs.shape, targets.shape, inputs.max(), inputs.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Riccardo\\Desktop\\Other\\package\\Indices_analysis\\notebooks\\.\n",
      "tensor(309.7275)\n",
      "tensor(309.7275)\n",
      "tensor(309.7275)\n",
      "tensor(309.7275)\n",
      "tensor(309.7275)\n",
      "tensor(309.7275)\n",
      "tensor(309.7275)\n",
      "tensor(309.7275)\n",
      "tensor(309.7275)\n",
      "tensor(309.7275)\n",
      "tensor(309.7275)\n",
      "tensor(309.7275)\n",
      "tensor(309.7275)\n",
      "tensor(309.7275)\n",
      "tensor(309.7275)\n",
      "tensor(309.7275)\n",
      "tensor(309.7275)\n",
      "tensor(309.7275)\n",
      "tensor(309.7275)\n",
      "tensor(309.7275)\n",
      "tensor(309.7275)\n",
      "tensor(309.7275)\n",
      "tensor(309.7275)\n",
      "tensor(309.7275)\n",
      "tensor(309.7275)\n",
      "tensor(309.7275)\n",
      "tensor(309.7275)\n",
      "tensor(309.7275)\n",
      "tensor(309.7275)\n",
      "tensor(309.7275)\n",
      "tensor(309.7275)\n",
      "tensor(309.7275)\n",
      "tensor(309.7275)\n",
      "tensor(309.7275)\n",
      "tensor(309.7275)\n",
      "tensor(309.7275)\n",
      "tensor(309.7275)\n",
      "tensor(309.7275)\n",
      "tensor(309.7275)\n",
      "tensor(309.7275)\n",
      "tensor(309.7275)\n",
      "tensor(309.7275)\n",
      "tensor(309.7275)\n",
      "tensor(309.7275)\n",
      "tensor(309.7275)\n",
      "tensor(309.7275)\n",
      "tensor(309.7275)\n",
      "tensor(309.7275)\n",
      "tensor(309.7275)\n",
      "tensor(309.7275)\n"
     ]
    }
   ],
   "source": [
    "#### Start training\n",
    "from p_drought_indices.configs.config_3x3_16_3x3_32_3x3_64 import config\n",
    "from torch.nn import MSELoss\n",
    "import matplotlib.pyplot as plt\n",
    "from p_drought_indices.analysis.DeepLearning.ConvLSTM import ConvLSTM, train_loop, valid_loop, build_logging\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "name = '3x3_16_3x3_32_3x3_64'\n",
    "\n",
    "logger = build_logging(config)\n",
    "model = ConvLSTM(config).to(config.device)\n",
    "#criterion = CrossEntropyLoss().to(config.device)\n",
    "#criterion = torch.nn.MSELoss().to(config.device)\n",
    "criterion = MSELoss().to(config.device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "train_records, valid_records, test_records = [], [], []\n",
    "for epoch in range(config.epochs):\n",
    "    epoch_records = train_loop(config, logger, epoch, model, train_dataloader, criterion, optimizer)\n",
    "    train_records.append(np.mean(epoch_records['loss']))\n",
    "    epoch_records = valid_loop(config, logger, epoch, model, test_dataloader, criterion)\n",
    "    valid_records.append(np.mean(epoch_records['loss']))\n",
    "    plt.plot(range(epoch + 1), train_records, label='train')\n",
    "    plt.plot(range(epoch + 1), valid_records, label='valid')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(config.output_dir, '{}.png'.format(name)))\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gis_py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
